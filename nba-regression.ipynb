{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function\n",
    "Consists of init, setting a learning rate, fitting, update the step. computing, caculating square loss and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class linear_regression_MultipleD:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.w = np.zeros(7 + 1) # Add one for the intercept term\n",
    "        self.loss_values = []\n",
    "        self.alpha = 1\n",
    "\n",
    "    def set_learning_rate(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, x, y, iteration=1500):\n",
    "        self.loss_values = []\n",
    "        self.y = y\n",
    "\n",
    "        self.x = np.c_[x, np.ones(x.shape[0])]\n",
    "\n",
    "        for i in range(iteration):\n",
    "            if not self.make_one_update():\n",
    "                self.set_learning_rate(self.alpha * 0.1)\n",
    "            self.loss_values.append(self.sq_loss(self.w))\n",
    "\n",
    "    def make_one_update(self):\n",
    "        w_current = self.w\n",
    "        step = (-1)*self.alpha*self.compute_gradient(w_current)\n",
    "        w_update = w_current + step\n",
    "        \n",
    "        current_loss = self.sq_loss(w_current)\n",
    "        update_loss = self.sq_loss(w_update)\n",
    "        if current_loss > update_loss:\n",
    "            print(\"Loss decreases to \", update_loss,)\n",
    "            self.w = w_update\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Loss increases to \", update_loss,)\n",
    "            return False\n",
    "\n",
    "    def compute_gradient(self, w_current):\n",
    "        grad_v = np.zeros(8)\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "\n",
    "        for j in range(8):\n",
    "            grad_v[j] = 2 / len(y) * np.sum((x.dot(w_current) - y) * x[:, j])\n",
    "\n",
    "        print(\"The norm of grad vector is \", np.sqrt(np.inner(grad_v, grad_v)))\n",
    "        return grad_v\n",
    "        \n",
    "    def sq_loss(self, w):\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        loss = np.sum((x.dot(w) - y) ** 2) / len(y)\n",
    "        return loss\n",
    "        pass\n",
    "\n",
    "    def calculate_rmse(self, y_pred, y_test):\n",
    "        y = self.y\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, test_size=0.2, random_state=None):\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        n_test_samples = int(test_size * n_samples)\n",
    "        \n",
    "        test_indices = indices[:n_test_samples]\n",
    "        train_indices = indices[n_test_samples:]    \n",
    "        \n",
    "        x_train = x[train_indices]\n",
    "        x_test = x[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "        \n",
    "        return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rk</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Precious Achiuwa</td>\n",
       "      <td>C</td>\n",
       "      <td>23</td>\n",
       "      <td>TOR</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>20.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Steven Adams</td>\n",
       "      <td>C</td>\n",
       "      <td>29</td>\n",
       "      <td>MEM</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bam Adebayo</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>MIA</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>34.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ochai Agbaji</td>\n",
       "      <td>SG</td>\n",
       "      <td>22</td>\n",
       "      <td>UTA</td>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santi Aldama</td>\n",
       "      <td>PF</td>\n",
       "      <td>22</td>\n",
       "      <td>MEM</td>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "      <td>21.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>535</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>34</td>\n",
       "      <td>TOR</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>14.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>536</td>\n",
       "      <td>Trae Young</td>\n",
       "      <td>PG</td>\n",
       "      <td>24</td>\n",
       "      <td>ATL</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>34.8</td>\n",
       "      <td>8.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>26.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>537</td>\n",
       "      <td>Omer Yurtseven</td>\n",
       "      <td>C</td>\n",
       "      <td>24</td>\n",
       "      <td>MIA</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>538</td>\n",
       "      <td>Cody Zeller</td>\n",
       "      <td>C</td>\n",
       "      <td>30</td>\n",
       "      <td>MIA</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>539</td>\n",
       "      <td>Ivica Zubac</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>LAC</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>28.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>679 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rk            Player Pos  Age   Tm   G  GS    MP   FG   FGA  ...    FT%  \\\n",
       "0      1  Precious Achiuwa   C   23  TOR  55  12  20.7  3.6   7.3  ...  0.702   \n",
       "1      2      Steven Adams   C   29  MEM  42  42  27.0  3.7   6.3  ...  0.364   \n",
       "2      3       Bam Adebayo   C   25  MIA  75  75  34.6  8.0  14.9  ...  0.806   \n",
       "3      4      Ochai Agbaji  SG   22  UTA  59  22  20.5  2.8   6.5  ...  0.812   \n",
       "4      5      Santi Aldama  PF   22  MEM  77  20  21.8  3.2   6.8  ...  0.750   \n",
       "..   ...               ...  ..  ...  ...  ..  ..   ...  ...   ...  ...    ...   \n",
       "674  535    Thaddeus Young  PF   34  TOR  54   9  14.7  2.0   3.7  ...  0.692   \n",
       "675  536        Trae Young  PG   24  ATL  73  73  34.8  8.2  19.0  ...  0.886   \n",
       "676  537    Omer Yurtseven   C   24  MIA   9   0   9.2  1.8   3.0  ...  0.833   \n",
       "677  538       Cody Zeller   C   30  MIA  15   2  14.5  2.5   3.9  ...  0.686   \n",
       "678  539       Ivica Zubac   C   25  LAC  76  76  28.6  4.3   6.8  ...  0.697   \n",
       "\n",
       "     ORB  DRB   TRB   AST  STL  BLK  TOV   PF   PTS  \n",
       "0    1.8  4.1   6.0   0.9  0.6  0.5  1.1  1.9   9.2  \n",
       "1    5.1  6.5  11.5   2.3  0.9  1.1  1.9  2.3   8.6  \n",
       "2    2.5  6.7   9.2   3.2  1.2  0.8  2.5  2.8  20.4  \n",
       "3    0.7  1.3   2.1   1.1  0.3  0.3  0.7  1.7   7.9  \n",
       "4    1.1  3.7   4.8   1.3  0.6  0.6  0.8  1.9   9.0  \n",
       "..   ...  ...   ...   ...  ...  ...  ...  ...   ...  \n",
       "674  1.3  1.8   3.1   1.4  1.0  0.1  0.8  1.6   4.4  \n",
       "675  0.8  2.2   3.0  10.2  1.1  0.1  4.1  1.4  26.2  \n",
       "676  0.9  1.7   2.6   0.2  0.2  0.2  0.4  1.8   4.4  \n",
       "677  1.7  2.6   4.3   0.7  0.2  0.3  0.9  2.2   6.5  \n",
       "678  3.1  6.8   9.9   1.0  0.4  1.3  1.5  2.9  10.8  \n",
       "\n",
       "[679 rows x 30 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('regular_season_2022-23.csv', encoding='ISO-8859-1', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rk', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%',\n",
       "       '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%',\n",
       "       'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for a null/missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rk        0\n",
       "Player    0\n",
       "Pos       0\n",
       "Age       0\n",
       "Tm        0\n",
       "G         0\n",
       "GS        0\n",
       "MP        0\n",
       "FG        0\n",
       "FGA       0\n",
       "FG%       0\n",
       "3P        0\n",
       "3PA       0\n",
       "3P%       0\n",
       "2P        0\n",
       "2PA       0\n",
       "2P%       0\n",
       "eFG%      0\n",
       "FT        0\n",
       "FTA       0\n",
       "FT%       0\n",
       "ORB       0\n",
       "DRB       0\n",
       "TRB       0\n",
       "AST       0\n",
       "STL       0\n",
       "BLK       0\n",
       "TOV       0\n",
       "PF        0\n",
       "PTS       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclusively utilize the following features for our linear regression analysis:\n",
    "1) MP: Minutes played per game.\n",
    "2) FG: Field goals made per game.\n",
    "3) FGA: Field goals attempted per game.\n",
    "4) FG%: Field goal percentage, i.e., the ratio of field goals made to attempted.\n",
    "5) 3P%: Three-point field goal percentage.\n",
    "6) 2P%: Two-point field goal percentage.\n",
    "7) FT%: Free throw percentage.\n",
    "\n",
    "The goal is to find a model that will estimate/predict each player point per game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['MP', 'FG', 'FGA', 'FG%', '3P%', '2P%', 'FT%']].values\n",
    "y = df['PTS'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  503.9584330653193\n",
      "Loss decreases to  10.264000503678545\n",
      "The norm of grad vector is  53.15441559663948\n",
      "Loss decreases to  8.894222349295605\n",
      "The norm of grad vector is  14.44704523938247\n",
      "Loss decreases to  8.701693015679794\n",
      "The norm of grad vector is  13.316777621049496\n",
      "Loss decreases to  8.525547161813911\n",
      "The norm of grad vector is  13.150846779535405\n",
      "Loss decreases to  8.353613307751758\n",
      "The norm of grad vector is  12.997243695279717\n",
      "Loss decreases to  8.18567076688197\n",
      "The norm of grad vector is  12.845553795313688\n",
      "Loss decreases to  8.021625353340236\n",
      "The norm of grad vector is  12.695646599392205\n",
      "Loss decreases to  7.861386336400365\n",
      "The norm of grad vector is  12.547500153807212\n",
      "Loss decreases to  7.704865110766185\n",
      "The norm of grad vector is  12.401093888649598\n",
      "Loss decreases to  7.5519751340945085\n",
      "The norm of grad vector is  12.256407487336118\n",
      "Loss decreases to  7.402631878832179\n",
      "The norm of grad vector is  12.11342087192173\n",
      "Loss decreases to  7.256752785308187\n",
      "The norm of grad vector is  11.972114200187928\n",
      "Loss decreases to  7.114257215919217\n",
      "The norm of grad vector is  11.832467862887333\n",
      "Loss decreases to  6.975066410381839\n",
      "The norm of grad vector is  11.69446248102189\n",
      "Loss decreases to  6.839103442026463\n",
      "The norm of grad vector is  11.558078903152959\n",
      "Loss decreases to  6.706293175108843\n",
      "The norm of grad vector is  11.42329820274296\n",
      "Loss decreases to  6.57656222311537\n",
      "The norm of grad vector is  11.29010167552818\n",
      "Loss decreases to  6.449838908039119\n",
      "The norm of grad vector is  11.158470836922346\n",
      "Loss decreases to  6.326053220603946\n",
      "The norm of grad vector is  11.028387419450699\n",
      "Loss decreases to  6.205136781414643\n",
      "The norm of grad vector is  10.899833370214074\n",
      "Loss decreases to  6.087022803011545\n",
      "The norm of grad vector is  10.772790848382753\n",
      "Loss decreases to  5.971646052808545\n",
      "The norm of grad vector is  10.647242222719694\n",
      "Loss decreases to  5.858942816893964\n",
      "The norm of grad vector is  10.523170069132766\n",
      "Loss decreases to  5.748850864674163\n",
      "The norm of grad vector is  10.400557168255693\n",
      "Loss decreases to  5.641309414340307\n",
      "The norm of grad vector is  10.279386503057369\n",
      "Loss decreases to  5.536259099139071\n",
      "The norm of grad vector is  10.159641256479164\n",
      "Loss decreases to  5.4336419344286435\n",
      "The norm of grad vector is  10.041304809099936\n",
      "Loss decreases to  5.333401285501657\n",
      "The norm of grad vector is  9.924360736828422\n",
      "Loss decreases to  5.235481836157267\n",
      "The norm of grad vector is  9.808792808622652\n",
      "Loss decreases to  5.139829558004896\n",
      "The norm of grad vector is  9.694584984236123\n",
      "Loss decreases to  5.046391680482619\n",
      "The norm of grad vector is  9.58172141199034\n",
      "Loss decreases to  4.955116661573555\n",
      "The norm of grad vector is  9.470186426573521\n",
      "Loss decreases to  4.865954159204015\n",
      "The norm of grad vector is  9.359964546865038\n",
      "Loss decreases to  4.778855003307522\n",
      "The norm of grad vector is  9.251040473785391\n",
      "Loss decreases to  4.693771168539235\n",
      "The norm of grad vector is  9.143399088171364\n",
      "Loss decreases to  4.610655747625574\n",
      "The norm of grad vector is  9.037025448676072\n",
      "Loss decreases to  4.529462925334338\n",
      "The norm of grad vector is  8.931904789693608\n",
      "Loss decreases to  4.450147953050783\n",
      "The norm of grad vector is  8.828022519308021\n",
      "Loss decreases to  4.372667123945637\n",
      "The norm of grad vector is  8.725364217266307\n",
      "Loss decreases to  4.296977748721194\n",
      "The norm of grad vector is  8.623915632975137\n",
      "Loss decreases to  4.2230381319221015\n",
      "The norm of grad vector is  8.52366268352107\n",
      "Loss decreases to  4.150807548797623\n",
      "The norm of grad vector is  8.424591451713933\n",
      "Loss decreases to  4.080246222702601\n",
      "The norm of grad vector is  8.32668818415312\n",
      "Loss decreases to  4.011315303024533\n",
      "The norm of grad vector is  8.229939289316542\n",
      "Loss decreases to  3.9439768436245366\n",
      "The norm of grad vector is  8.134331335671943\n",
      "Loss decreases to  3.878193781780227\n",
      "The norm of grad vector is  8.039851049810322\n",
      "Loss decreases to  3.8139299176188377\n",
      "The norm of grad vector is  7.946485314601222\n",
      "Loss decreases to  3.7511498940291435\n",
      "The norm of grad vector is  7.854221167369594\n",
      "Loss decreases to  3.6898191770410764\n",
      "The norm of grad vector is  7.763045798094002\n",
      "Loss decreases to  3.629904036662106\n",
      "The norm of grad vector is  7.672946547625911\n",
      "Loss decreases to  3.571371528159783\n",
      "The norm of grad vector is  7.583910905929812\n",
      "Loss decreases to  3.5141894737800383\n",
      "The norm of grad vector is  7.495926510343913\n",
      "Loss decreases to  3.458326444891105\n",
      "The norm of grad vector is  7.408981143861206\n",
      "Loss decreases to  3.403751744543145\n",
      "The norm of grad vector is  7.323062733430609\n",
      "Loss decreases to  3.3504353904339093\n",
      "The norm of grad vector is  7.238159348277972\n",
      "Loss decreases to  3.298348098270975\n",
      "The norm of grad vector is  7.15425919824671\n",
      "Loss decreases to  3.247461265521326\n",
      "The norm of grad vector is  7.071350632157845\n",
      "Loss decreases to  3.1977469555392664\n",
      "The norm of grad vector is  6.989422136189172\n",
      "Loss decreases to  3.1491778820638454\n",
      "The norm of grad vector is  6.9084623322734\n",
      "Loss decreases to  3.101727394077207\n",
      "The norm of grad vector is  6.828459976514966\n",
      "Loss decreases to  3.05536946101545\n",
      "The norm of grad vector is  6.749403957625355\n",
      "Loss decreases to  3.010078658323791\n",
      "The norm of grad vector is  6.671283295376671\n",
      "Loss decreases to  2.9658301533480307\n",
      "The norm of grad vector is  6.594087139073255\n",
      "Loss decreases to  2.9225996915544665\n",
      "The norm of grad vector is  6.517804766041129\n",
      "Loss decreases to  2.8803635830706362\n",
      "The norm of grad vector is  6.442425580135065\n",
      "Loss decreases to  2.8390986895393944\n",
      "The norm of grad vector is  6.367939110263049\n",
      "Loss decreases to  2.798782411279052\n",
      "The norm of grad vector is  6.294335008927934\n",
      "Loss decreases to  2.7593926747424304\n",
      "The norm of grad vector is  6.221603050786093\n",
      "Loss decreases to  2.7209079202678943\n",
      "The norm of grad vector is  6.1497331312228525\n",
      "Loss decreases to  2.6833070901155374\n",
      "The norm of grad vector is  6.078715264944494\n",
      "Loss decreases to  2.6465696167819233\n",
      "The norm of grad vector is  6.00853958458666\n",
      "Loss decreases to  2.6106754115868434\n",
      "The norm of grad vector is  5.939196339338921\n",
      "Loss decreases to  2.5756048535258116\n",
      "The norm of grad vector is  5.8706758935853385\n",
      "Loss decreases to  2.541338778382069\n",
      "The norm of grad vector is  5.802968725560829\n",
      "Loss decreases to  2.5078584680920804\n",
      "The norm of grad vector is  5.7360654260231065\n",
      "Loss decreases to  2.4751456403586154\n",
      "The norm of grad vector is  5.66995669694007\n",
      "Loss decreases to  2.4431824385056404\n",
      "The norm of grad vector is  5.604633350192386\n",
      "Loss decreases to  2.411951421569406\n",
      "The norm of grad vector is  5.540086306291126\n",
      "Loss decreases to  2.3814355546202255\n",
      "The norm of grad vector is  5.476306593110253\n",
      "Loss decreases to  2.3516181993095655\n",
      "The norm of grad vector is  5.413285344633778\n",
      "Loss decreases to  2.322483104637219\n",
      "The norm of grad vector is  5.351013799717423\n",
      "Loss decreases to  2.2940143979334255\n",
      "The norm of grad vector is  5.289483300864582\n",
      "Loss decreases to  2.2661965760509353\n",
      "The norm of grad vector is  5.228685293016438\n",
      "Loss decreases to  2.2390144967621306\n",
      "The norm of grad vector is  5.16861132235602\n",
      "Loss decreases to  2.2124533703564424\n",
      "The norm of grad vector is  5.109253035126081\n",
      "Loss decreases to  2.1864987514333714\n",
      "The norm of grad vector is  5.050602176460582\n",
      "Loss decreases to  2.16113653088659\n",
      "The norm of grad vector is  4.992650589229619\n",
      "Loss decreases to  2.1363529280746554\n",
      "The norm of grad vector is  4.93539021289765\n",
      "Loss decreases to  2.112134483174004\n",
      "The norm of grad vector is  4.878813082394837\n",
      "Loss decreases to  2.088468049709969\n",
      "The norm of grad vector is  4.822911327001337\n",
      "Loss decreases to  2.0653407872616847\n",
      "The norm of grad vector is  4.767677169244386\n",
      "Loss decreases to  2.042740154336839\n",
      "The norm of grad vector is  4.7131029238080195\n",
      "Loss decreases to  2.020653901412295\n",
      "The norm of grad vector is  4.65918099645526\n",
      "Loss decreases to  1.9990700641367412\n",
      "The norm of grad vector is  4.6059038829626155\n",
      "Loss decreases to  1.977976956691595\n",
      "The norm of grad vector is  4.5532641680667485\n",
      "Loss decreases to  1.9573631653064647\n",
      "The norm of grad vector is  4.501254524423134\n",
      "Loss decreases to  1.937217541925582\n",
      "The norm of grad vector is  4.449867711576575\n",
      "Loss decreases to  1.917529198021688\n",
      "The norm of grad vector is  4.3990965749434165\n",
      "Loss decreases to  1.8982874985539364\n",
      "The norm of grad vector is  4.348934044805302\n",
      "Loss decreases to  1.8794820560664622\n",
      "The norm of grad vector is  4.299373135314345\n",
      "Loss decreases to  1.861102724924346\n",
      "The norm of grad vector is  4.2504069435095255\n",
      "Loss decreases to  1.8431395956837702\n",
      "The norm of grad vector is  4.202028648344223\n",
      "Loss decreases to  1.8255829895932398\n",
      "The norm of grad vector is  4.154231509724689\n",
      "Loss decreases to  1.8084234532228267\n",
      "The norm of grad vector is  4.107008867559351\n",
      "Loss decreases to  1.7916517532184482\n",
      "The norm of grad vector is  4.060354140818782\n",
      "Loss decreases to  1.7752588711782697\n",
      "The norm of grad vector is  4.014260826606218\n",
      "Loss decreases to  1.7592359986483952\n",
      "The norm of grad vector is  3.968722499238464\n",
      "Loss decreases to  1.7435745322350589\n",
      "The norm of grad vector is  3.923732809337058\n",
      "Loss decreases to  1.7282660688306128\n",
      "The norm of grad vector is  3.879285482929569\n",
      "Loss decreases to  1.713302400950658\n",
      "The norm of grad vector is  3.835374320560865\n",
      "Loss decreases to  1.6986755121797321\n",
      "The norm of grad vector is  3.7919931964142486\n",
      "Loss decreases to  1.6843775727230266\n",
      "The norm of grad vector is  3.7491360574423003\n",
      "Loss decreases to  1.6704009350616629\n",
      "The norm of grad vector is  3.7067969225073214\n",
      "Loss decreases to  1.656738129709125\n",
      "The norm of grad vector is  3.6649698815312286\n",
      "Loss decreases to  1.6433818610664772\n",
      "The norm of grad vector is  3.623649094654777\n",
      "Loss decreases to  1.630325003374084\n",
      "The norm of grad vector is  3.5828287914059875\n",
      "Loss decreases to  1.617560596757576\n",
      "The norm of grad vector is  3.542503269877641\n",
      "Loss decreases to  1.6050818433658716\n",
      "The norm of grad vector is  3.502666895913736\n",
      "Loss decreases to  1.592882103599104\n",
      "The norm of grad vector is  3.4633141023047473\n",
      "Loss decreases to  1.5809548924243761\n",
      "The norm of grad vector is  3.424439387991606\n",
      "Loss decreases to  1.5692938757772794\n",
      "The norm of grad vector is  3.3860373172782525\n",
      "Loss decreases to  1.5578928670471943\n",
      "The norm of grad vector is  3.34810251905264\n",
      "Loss decreases to  1.5467458236444134\n",
      "The norm of grad vector is  3.3106296860160835\n",
      "Loss decreases to  1.5358468436471915\n",
      "The norm of grad vector is  3.2736135739208265\n",
      "Loss decreases to  1.5251901625268474\n",
      "The norm of grad vector is  3.237049000815705\n",
      "Loss decreases to  1.5147701499491184\n",
      "The norm of grad vector is  3.2009308462998107\n",
      "Loss decreases to  1.5045813066499734\n",
      "The norm of grad vector is  3.1652540507840072\n",
      "Loss decreases to  1.4946182613841685\n",
      "The norm of grad vector is  3.1300136147602156\n",
      "Loss decreases to  1.484875767944837\n",
      "The norm of grad vector is  3.0952045980783445\n",
      "Loss decreases to  1.4753487022524723\n",
      "The norm of grad vector is  3.060822119230741\n",
      "Loss decreases to  1.4660320595116778\n",
      "The norm of grad vector is  3.026861354644079\n",
      "Loss decreases to  1.4569209514341135\n",
      "The norm of grad vector is  2.9933175379785495\n",
      "Loss decreases to  1.4480106035260953\n",
      "The norm of grad vector is  2.960185959434269\n",
      "Loss decreases to  1.4392963524393398\n",
      "The norm of grad vector is  2.9274619650647726\n",
      "Loss decreases to  1.4307736433833877\n",
      "The norm of grad vector is  2.895140956097506\n",
      "Loss decreases to  1.4224380275982713\n",
      "The norm of grad vector is  2.863218388261214\n",
      "Loss decreases to  1.4142851598860167\n",
      "The norm of grad vector is  2.8316897711201077\n",
      "Loss decreases to  1.4063107961996195\n",
      "The norm of grad vector is  2.8005506674147194\n",
      "Loss decreases to  1.398510791288147\n",
      "The norm of grad vector is  2.769796692409344\n",
      "Loss decreases to  1.3908810963966705\n",
      "The norm of grad vector is  2.7394235132459612\n",
      "Loss decreases to  1.3834177570197415\n",
      "The norm of grad vector is  2.709426848304555\n",
      "Loss decreases to  1.37611691070717\n",
      "The norm of grad vector is  2.6798024665697215\n",
      "Loss decreases to  1.3689747849208898\n",
      "The norm of grad vector is  2.6505461870034717\n",
      "Loss decreases to  1.3619876949417133\n",
      "The norm of grad vector is  2.621653877924146\n",
      "Loss decreases to  1.355152041824824\n",
      "The norm of grad vector is  2.5931214563913403\n",
      "Loss decreases to  1.348464310402864\n",
      "The norm of grad vector is  2.5649448875967495\n",
      "Loss decreases to  1.3419210673355124\n",
      "The norm of grad vector is  2.5371201842608495\n",
      "Loss decreases to  1.3355189592044767\n",
      "The norm of grad vector is  2.509643406035342\n",
      "Loss decreases to  1.3292547106528239\n",
      "The norm of grad vector is  2.482510658911228\n",
      "Loss decreases to  1.323125122567642\n",
      "The norm of grad vector is  2.4557180946324975\n",
      "Loss decreases to  1.3171270703050013\n",
      "The norm of grad vector is  2.4292619101152884\n",
      "Loss decreases to  1.3112575019562498\n",
      "The norm of grad vector is  2.4031383468724803\n",
      "Loss decreases to  1.305513436654662\n",
      "The norm of grad vector is  2.3773436904436203\n",
      "Loss decreases to  1.299891962921516\n",
      "The norm of grad vector is  2.3518742698301076\n",
      "Loss decreases to  1.2943902370506772\n",
      "The norm of grad vector is  2.3267264569355746\n",
      "Loss decreases to  1.2890054815307845\n",
      "The norm of grad vector is  2.3018966660113818\n",
      "Loss decreases to  1.2837349835041743\n",
      "The norm of grad vector is  2.277381353107147\n",
      "Loss decreases to  1.2785760932616839\n",
      "The norm of grad vector is  2.2531770155262727\n",
      "Loss decreases to  1.2735262227724922\n",
      "The norm of grad vector is  2.2292801912863602\n",
      "Loss decreases to  1.2685828442481915\n",
      "The norm of grad vector is  2.2056874585844963\n",
      "Loss decreases to  1.263743488740287\n",
      "The norm of grad vector is  2.182395435267295\n",
      "Loss decreases to  1.259005744770349\n",
      "The norm of grad vector is  2.1594007783056943\n",
      "Loss decreases to  1.2543672569920596\n",
      "The norm of grad vector is  2.1367001832744044\n",
      "Loss decreases to  1.2498257248843998\n",
      "The norm of grad vector is  2.1142903838359723\n",
      "Loss decreases to  1.2453789014752705\n",
      "The norm of grad vector is  2.0921681512294112\n",
      "Loss decreases to  1.241024592094824\n",
      "The norm of grad vector is  2.0703302937633454\n",
      "Loss decreases to  1.2367606531578186\n",
      "The norm of grad vector is  2.048773656313614\n",
      "Loss decreases to  1.2325849909743227\n",
      "The norm of grad vector is  2.0274951198252937\n",
      "Loss decreases to  1.2284955605881094\n",
      "The norm of grad vector is  2.006491600819119\n",
      "Loss decreases to  1.2244903646420884\n",
      "The norm of grad vector is  1.9857600509022233\n",
      "Loss decreases to  1.2205674522701595\n",
      "The norm of grad vector is  1.9652974562831955\n",
      "Loss decreases to  1.2167249180148574\n",
      "The norm of grad vector is  1.9451008372914063\n",
      "Loss decreases to  1.2129609007702005\n",
      "The norm of grad vector is  1.9251672479005826\n",
      "Loss decreases to  1.209273582749149\n",
      "The norm of grad vector is  1.9054937752565886\n",
      "Loss decreases to  1.2056611884751016\n",
      "The norm of grad vector is  1.8860775392094125\n",
      "Loss decreases to  1.2021219837968729\n",
      "The norm of grad vector is  1.8669156918493055\n",
      "Loss decreases to  1.1986542749265992\n",
      "The norm of grad vector is  1.8480054170470952\n",
      "Loss decreases to  1.1952564075000491\n",
      "The norm of grad vector is  1.8293439299986258\n",
      "Loss decreases to  1.1919267656588048\n",
      "The norm of grad vector is  1.8109284767733314\n",
      "Loss decreases to  1.1886637711538166\n",
      "The norm of grad vector is  1.7927563338669337\n",
      "Loss decreases to  1.1854658824698219\n",
      "The norm of grad vector is  1.7748248077582482\n",
      "Loss decreases to  1.1823315939701537\n",
      "The norm of grad vector is  1.7571312344701155\n",
      "Loss decreases to  1.1792594350614574\n",
      "The norm of grad vector is  1.7396729791344434\n",
      "Loss decreases to  1.1762479693778531\n",
      "The norm of grad vector is  1.7224474355613846\n",
      "Loss decreases to  1.1732957939840907\n",
      "The norm of grad vector is  1.7054520258126338\n",
      "Loss decreases to  1.1704015385972593\n",
      "The norm of grad vector is  1.6886841997788826\n",
      "Loss decreases to  1.1675638648266122\n",
      "The norm of grad vector is  1.6721414347614292\n",
      "Loss decreases to  1.1647814654310946\n",
      "The norm of grad vector is  1.6558212350579724\n",
      "Loss decreases to  1.1620530635941513\n",
      "The norm of grad vector is  1.6397211315526166\n",
      "Loss decreases to  1.1593774122154212\n",
      "The norm of grad vector is  1.623838681310094\n",
      "Loss decreases to  1.156753293218922\n",
      "The norm of grad vector is  1.6081714671742615\n",
      "Loss decreases to  1.1541795168773383\n",
      "The norm of grad vector is  1.59271709737088\n",
      "Loss decreases to  1.1516549211520446\n",
      "The norm of grad vector is  1.5774732051147322\n",
      "Loss decreases to  1.1491783710484904\n",
      "The norm of grad vector is  1.5624374482210968\n",
      "Loss decreases to  1.1467487579865936\n",
      "The norm of grad vector is  1.5476075087216465\n",
      "Loss decreases to  1.144364999185794\n",
      "The norm of grad vector is  1.5329810924847895\n",
      "Loss decreases to  1.142026037064417\n",
      "The norm of grad vector is  1.5185559288405264\n",
      "Loss decreases to  1.1397308386530307\n",
      "The norm of grad vector is  1.5043297702098521\n",
      "Loss decreases to  1.1374783950214507\n",
      "The norm of grad vector is  1.4903003917387692\n",
      "Loss decreases to  1.1352677207190902\n",
      "The norm of grad vector is  1.4764655909369675\n",
      "Loss decreases to  1.1330978532283407\n",
      "The norm of grad vector is  1.4628231873212243\n",
      "Loss decreases to  1.1309678524306743\n",
      "The norm of grad vector is  1.4493710220635894\n",
      "Loss decreases to  1.128876800085176\n",
      "The norm of grad vector is  1.436106957644412\n",
      "Loss decreases to  1.1268237993192218\n",
      "The norm of grad vector is  1.4230288775102855\n",
      "Loss decreases to  1.1248079741310084\n",
      "The norm of grad vector is  1.4101346857369612\n",
      "Loss decreases to  1.1228284689036667\n",
      "The norm of grad vector is  1.3974223066973162\n",
      "Loss decreases to  1.1208844479306876\n",
      "The norm of grad vector is  1.3848896847344216\n",
      "Loss decreases to  1.1189750949523958\n",
      "The norm of grad vector is  1.3725347838398116\n",
      "Loss decreases to  1.1170996127032158\n",
      "The norm of grad vector is  1.360355587336995\n",
      "Loss decreases to  1.1152572224694783\n",
      "The norm of grad vector is  1.3483500975702956\n",
      "Loss decreases to  1.1134471636575167\n",
      "The norm of grad vector is  1.3365163355990952\n",
      "Loss decreases to  1.1116686933718276\n",
      "The norm of grad vector is  1.3248523408975421\n",
      "Loss decreases to  1.1099210860030437\n",
      "The norm of grad vector is  1.3133561710598056\n",
      "Loss decreases to  1.108203632825504\n",
      "The norm of grad vector is  1.3020259015109354\n",
      "Loss decreases to  1.1065156416041915\n",
      "The norm of grad vector is  1.290859625223416\n",
      "Loss decreases to  1.1048564362108215\n",
      "The norm of grad vector is  1.279855452439461\n",
      "Loss decreases to  1.103225356248868\n",
      "The norm of grad vector is  1.2690115103991335\n",
      "Loss decreases to  1.1016217566873214\n",
      "The norm of grad vector is  1.2583259430743556\n",
      "Loss decreases to  1.1000450075029693\n",
      "The norm of grad vector is  1.2477969109088543\n",
      "Loss decreases to  1.09849449333101\n",
      "The norm of grad vector is  1.2374225905641332\n",
      "Loss decreases to  1.096969613123794\n",
      "The norm of grad vector is  1.227201174671503\n",
      "Loss decreases to  1.0954697798175184\n",
      "The norm of grad vector is  1.2171308715902431\n",
      "Loss decreases to  1.0939944200066718\n",
      "The norm of grad vector is  1.2072099051719345\n",
      "Loss decreases to  1.0925429736260677\n",
      "The norm of grad vector is  1.1974365145310246\n",
      "Loss decreases to  1.091114893640272\n",
      "The norm of grad vector is  1.1878089538216585\n",
      "Loss decreases to  1.0897096457402669\n",
      "The norm of grad vector is  1.1783254920208177\n",
      "Loss decreases to  1.0883267080471761\n",
      "The norm of grad vector is  1.1689844127178135\n",
      "Loss decreases to  1.0869655708228834\n",
      "The norm of grad vector is  1.1597840139101439\n",
      "Loss decreases to  1.0856257361873929\n",
      "The norm of grad vector is  1.150722607805763\n",
      "Loss decreases to  1.0843067178427679\n",
      "The norm of grad vector is  1.141798520631766\n",
      "Loss decreases to  1.083008040803497\n",
      "The norm of grad vector is  1.1330100924495057\n",
      "Loss decreases to  1.0817292411331352\n",
      "The norm of grad vector is  1.124355676976156\n",
      "Loss decreases to  1.0804698656870806\n",
      "The norm of grad vector is  1.1158336414127203\n",
      "Loss decreases to  1.0792294718613367\n",
      "The norm of grad vector is  1.107442366278471\n",
      "Loss decreases to  1.0780076273471229\n",
      "The norm of grad vector is  1.0991802452518211\n",
      "Loss decreases to  1.0768039098912012\n",
      "The norm of grad vector is  1.0910456850176005\n",
      "Loss decreases to  1.07561790706178\n",
      "The norm of grad vector is  1.083037105120713\n",
      "Loss decreases to  1.0744492160198682\n",
      "The norm of grad vector is  1.075152937826139\n",
      "Loss decreases to  1.0732974432959546\n",
      "The norm of grad vector is  1.067391627985251\n",
      "Loss decreases to  1.072162204571883\n",
      "The norm of grad vector is  1.0597516329083732\n",
      "Loss decreases to  1.0710431244678087\n",
      "The norm of grad vector is  1.0522314222435558\n",
      "Loss decreases to  1.0699398363341133\n",
      "The norm of grad vector is  1.0448294778614775\n",
      "Loss decreases to  1.0688519820481632\n",
      "The norm of grad vector is  1.0375442937464152\n",
      "Loss decreases to  1.067779211815802\n",
      "The norm of grad vector is  1.0303743758932005\n",
      "Loss decreases to  1.0667211839774617\n",
      "The norm of grad vector is  1.023318242210068\n",
      "Loss decreases to  1.0656775648187902\n",
      "The norm of grad vector is  1.0163744224273183\n",
      "Loss decreases to  1.0646480283856874\n",
      "The norm of grad vector is  1.0095414580116653\n",
      "Loss decreases to  1.0636322563036433\n",
      "The norm of grad vector is  1.0028179020861854\n",
      "Loss decreases to  1.0626299376012904\n",
      "The norm of grad vector is  0.99620231935573\n",
      "Loss decreases to  1.0616407685380542\n",
      "The norm of grad vector is  0.9896932860376871\n",
      "Loss decreases to  1.0606644524358249\n",
      "The norm of grad vector is  0.9832893897979661\n",
      "Loss decreases to  1.059700699514541\n",
      "The norm of grad vector is  0.9769892296920389\n",
      "Loss decreases to  1.0587492267316057\n",
      "The norm of grad vector is  0.9707914161109262\n",
      "Loss decreases to  1.0578097576250396\n",
      "The norm of grad vector is  0.9646945707319563\n",
      "Loss decreases to  1.0568820221602844\n",
      "The norm of grad vector is  0.9586973264741403\n",
      "Loss decreases to  1.0559657565805747\n",
      "The norm of grad vector is  0.9527983274580073\n",
      "Loss decreases to  1.055060703260793\n",
      "The norm of grad vector is  0.9469962289697222\n",
      "Loss decreases to  1.0541666105647263\n",
      "The norm of grad vector is  0.9412896974293147\n",
      "Loss decreases to  1.0532832327056454\n",
      "The norm of grad vector is  0.935677410362842\n",
      "Loss decreases to  1.052410329610133\n",
      "The norm of grad vector is  0.9301580563782967\n",
      "Loss decreases to  1.0515476667850787\n",
      "The norm of grad vector is  0.9247303351450792\n",
      "Loss decreases to  1.050695015187772\n",
      "The norm of grad vector is  0.9193929573768335\n",
      "Loss decreases to  1.04985215109902\n",
      "The norm of grad vector is  0.9141446448174637\n",
      "Loss decreases to  1.0490188559992177\n",
      "The norm of grad vector is  0.9089841302301306\n",
      "Loss decreases to  1.0481949164473054\n",
      "The norm of grad vector is  0.9039101573890099\n",
      "Loss decreases to  1.047380123962542\n",
      "The norm of grad vector is  0.8989214810736631\n",
      "Loss decreases to  1.0465742749090323\n",
      "The norm of grad vector is  0.894016867065742\n",
      "Loss decreases to  1.0457771703829428\n",
      "The norm of grad vector is  0.8891950921479038\n",
      "Loss decreases to  1.0449886161023405\n",
      "The norm of grad vector is  0.8844549441046745\n",
      "Loss decreases to  1.0442084222996015\n",
      "The norm of grad vector is  0.8797952217250826\n",
      "Loss decreases to  1.043436403616318\n",
      "The norm of grad vector is  0.8752147348068712\n",
      "Loss decreases to  1.0426723790006582\n",
      "The norm of grad vector is  0.8707123041620504\n",
      "Loss decreases to  1.041916171607111\n",
      "The norm of grad vector is  0.8662867616236325\n",
      "Loss decreases to  1.0411676086985666\n",
      "The norm of grad vector is  0.8619369500533285\n",
      "Loss decreases to  1.0404265215506787\n",
      "The norm of grad vector is  0.8576617233500051\n",
      "Loss decreases to  1.0396927453584488\n",
      "The norm of grad vector is  0.8534599464587352\n",
      "Loss decreases to  1.0389661191449906\n",
      "The norm of grad vector is  0.8493304953802243\n",
      "Loss decreases to  1.0382464856724123\n",
      "The norm of grad vector is  0.8452722571804463\n",
      "Loss decreases to  1.0375336913547812\n",
      "The norm of grad vector is  0.8412841300003022\n",
      "Loss decreases to  1.036827586173106\n",
      "The norm of grad vector is  0.8373650230651145\n",
      "Loss decreases to  1.036128023592306\n",
      "The norm of grad vector is  0.8335138566938081\n",
      "Loss decreases to  1.0354348604801094\n",
      "The norm of grad vector is  0.8297295623075824\n",
      "Loss decreases to  1.0347479570278395\n",
      "The norm of grad vector is  0.8260110824379345\n",
      "Loss decreases to  1.0340671766730472\n",
      "The norm of grad vector is  0.8223573707338708\n",
      "Loss decreases to  1.0333923860239427\n",
      "The norm of grad vector is  0.8187673919681451\n",
      "Loss decreases to  1.032723454785589\n",
      "The norm of grad vector is  0.8152401220424048\n",
      "Loss decreases to  1.0320602556878087\n",
      "The norm of grad vector is  0.8117745479910784\n",
      "Loss decreases to  1.0314026644147771\n",
      "The norm of grad vector is  0.8083696679838993\n",
      "Loss decreases to  1.0307505595362458\n",
      "The norm of grad vector is  0.8050244913269184\n",
      "Loss decreases to  1.0301038224403731\n",
      "The norm of grad vector is  0.8017380384619067\n",
      "Loss decreases to  1.0294623372681184\n",
      "The norm of grad vector is  0.7985093409640303\n",
      "Loss decreases to  1.0288259908491586\n",
      "The norm of grad vector is  0.7953374415376874\n",
      "Loss decreases to  1.0281946726393023\n",
      "The norm of grad vector is  0.792221394010431\n",
      "Loss decreases to  1.0275682746593606\n",
      "The norm of grad vector is  0.7891602633248581\n",
      "Loss decreases to  1.0269466914354344\n",
      "The norm of grad vector is  0.7861531255284196\n",
      "Loss decreases to  1.0263298199406004\n",
      "The norm of grad vector is  0.7831990677610423\n",
      "Loss decreases to  1.0257175595379504\n",
      "The norm of grad vector is  0.7802971882405236\n",
      "Loss decreases to  1.0251098119249573\n",
      "The norm of grad vector is  0.7774465962456198\n",
      "Loss decreases to  1.024506481079134\n",
      "The norm of grad vector is  0.7746464120967843\n",
      "Loss decreases to  1.0239074732049633\n",
      "The norm of grad vector is  0.7718957671345116\n",
      "Loss decreases to  1.0233126966820576\n",
      "The norm of grad vector is  0.769193803695234\n",
      "Loss decreases to  1.0227220620145288\n",
      "The norm of grad vector is  0.7665396750847592\n",
      "Loss decreases to  1.0221354817815342\n",
      "The norm of grad vector is  0.7639325455492081\n",
      "Loss decreases to  1.021552870588976\n",
      "The norm of grad vector is  0.761371590243444\n",
      "Loss decreases to  1.0209741450223262\n",
      "The norm of grad vector is  0.7588559951969664\n",
      "Loss decreases to  1.0203992236005457\n",
      "The norm of grad vector is  0.7563849572772788\n",
      "Loss decreases to  1.019828026731079\n",
      "The norm of grad vector is  0.7539576841507263\n",
      "Loss decreases to  1.0192604766658993\n",
      "The norm of grad vector is  0.7515733942407928\n",
      "Loss decreases to  1.018696497458569\n",
      "The norm of grad vector is  0.7492313166838974\n",
      "Loss decreases to  1.0181360149223098\n",
      "The norm of grad vector is  0.7469306912826765\n",
      "Loss decreases to  1.0175789565890394\n",
      "The norm of grad vector is  0.744670768456797\n",
      "Loss decreases to  1.017025251669371\n",
      "The norm of grad vector is  0.742450809191309\n",
      "Loss decreases to  1.0164748310135352\n",
      "The norm of grad vector is  0.7402700849825815\n",
      "Loss decreases to  1.0159276270732192\n",
      "The norm of grad vector is  0.7381278777818497\n",
      "Loss decreases to  1.0153835738642891\n",
      "The norm of grad vector is  0.736023479936417\n",
      "Loss decreases to  1.014842606930383\n",
      "The norm of grad vector is  0.7339561941285506\n",
      "Loss decreases to  1.0143046633073516\n",
      "The norm of grad vector is  0.7319253333121284\n",
      "Loss decreases to  1.0137696814885242\n",
      "The norm of grad vector is  0.7299302206470699\n",
      "Loss decreases to  1.013237601390786\n",
      "The norm of grad vector is  0.7279701894316274\n",
      "Loss decreases to  1.0127083643214438\n",
      "The norm of grad vector is  0.7260445830325764\n",
      "Loss decreases to  1.0121819129458638\n",
      "The norm of grad vector is  0.7241527548133658\n",
      "Loss decreases to  1.0116581912558624\n",
      "The norm of grad vector is  0.7222940680603129\n",
      "Loss decreases to  1.0111371445388324\n",
      "The norm of grad vector is  0.7204678959068699\n",
      "Loss decreases to  1.0106187193475895\n",
      "The norm of grad vector is  0.7186736212560603\n",
      "Loss decreases to  1.010102863470919\n",
      "The norm of grad vector is  0.7169106367011362\n",
      "Loss decreases to  1.00958952590481\n",
      "The norm of grad vector is  0.7151783444445345\n",
      "Loss decreases to  1.0090786568243564\n",
      "The norm of grad vector is  0.7134761562151972\n",
      "Loss decreases to  1.0085702075563163\n",
      "The norm of grad vector is  0.7118034931843292\n",
      "Loss decreases to  1.008064130552306\n",
      "The norm of grad vector is  0.7101597858796662\n",
      "Loss decreases to  1.0075603793626213\n",
      "The norm of grad vector is  0.7085444740983351\n",
      "Loss decreases to  1.007058908610667\n",
      "The norm of grad vector is  0.7069570068183633\n",
      "Loss decreases to  1.0065596739679796\n",
      "The norm of grad vector is  0.7053968421089293\n",
      "Loss decreases to  1.0060626321298374\n",
      "The norm of grad vector is  0.7038634470394177\n",
      "Loss decreases to  1.0055677407914299\n",
      "The norm of grad vector is  0.7023562975873613\n",
      "Loss decreases to  1.0050749586245902\n",
      "The norm of grad vector is  0.7008748785453366\n",
      "Loss decreases to  1.0045842452550642\n",
      "The norm of grad vector is  0.6994186834268936\n",
      "Loss decreases to  1.00409556124031\n",
      "The norm of grad vector is  0.6979872143715928\n",
      "Loss decreases to  1.003608868047816\n",
      "The norm of grad vector is  0.696579982049219\n",
      "Loss decreases to  1.0031241280339194\n",
      "The norm of grad vector is  0.6951965055632506\n",
      "Loss decreases to  1.002641304423124\n",
      "The norm of grad vector is  0.6938363123536481\n",
      "Loss decreases to  1.0021603612878947\n",
      "The norm of grad vector is  0.6924989380990411\n",
      "Loss decreases to  1.0016812635289218\n",
      "The norm of grad vector is  0.6911839266183782\n",
      "Loss decreases to  1.0012039768558514\n",
      "The norm of grad vector is  0.68989082977211\n",
      "Loss decreases to  1.0007284677684551\n",
      "The norm of grad vector is  0.6886192073629727\n",
      "Loss decreases to  1.0002547035382465\n",
      "The norm of grad vector is  0.6873686270364385\n",
      "Loss decreases to  0.9997826521905187\n",
      "The norm of grad vector is  0.6861386641808975\n",
      "Loss decreases to  0.9993122824868067\n",
      "The norm of grad vector is  0.6849289018276367\n",
      "Loss decreases to  0.9988435639077522\n",
      "The norm of grad vector is  0.6837389305506718\n",
      "Loss decreases to  0.9983764666363709\n",
      "The norm of grad vector is  0.682568348366501\n",
      "Loss decreases to  0.9979109615417071\n",
      "The norm of grad vector is  0.6814167606338306\n",
      "Loss decreases to  0.9974470201628712\n",
      "The norm of grad vector is  0.6802837799533379\n",
      "Loss decreases to  0.9969846146934477\n",
      "The norm of grad vector is  0.6791690260675202\n",
      "Loss decreases to  0.9965237179662644\n",
      "The norm of grad vector is  0.6780721257606874\n",
      "Loss decreases to  0.9960643034385203\n",
      "The norm of grad vector is  0.676992712759147\n",
      "Loss decreases to  0.9956063451772541\n",
      "The norm of grad vector is  0.6759304276316395\n",
      "Loss decreases to  0.9951498178451572\n",
      "The norm of grad vector is  0.6748849176900596\n",
      "Loss decreases to  0.9946946966867128\n",
      "The norm of grad vector is  0.6738558368905279\n",
      "Loss decreases to  0.9942409575146589\n",
      "The norm of grad vector is  0.6728428457348383\n",
      "Loss decreases to  0.9937885766967658\n",
      "The norm of grad vector is  0.6718456111723383\n",
      "Loss decreases to  0.9933375311429214\n",
      "The norm of grad vector is  0.6708638065022832\n",
      "Loss decreases to  0.9928877982925219\n",
      "The norm of grad vector is  0.6698971112766943\n",
      "Loss decreases to  0.9924393561021473\n",
      "The norm of grad vector is  0.668945211203772\n",
      "Loss decreases to  0.9919921830335289\n",
      "The norm of grad vector is  0.6680077980518863\n",
      "Loss decreases to  0.9915462580418\n",
      "The norm of grad vector is  0.6670845695541946\n",
      "Loss decreases to  0.9911015605640129\n",
      "The norm of grad vector is  0.666175229313907\n",
      "Loss decreases to  0.9906580705079293\n",
      "The norm of grad vector is  0.6652794867102351\n",
      "Loss decreases to  0.9902157682410673\n",
      "The norm of grad vector is  0.6643970568050598\n",
      "Loss decreases to  0.9897746345800075\n",
      "The norm of grad vector is  0.6635276602503313\n",
      "Loss decreases to  0.9893346507799438\n",
      "The norm of grad vector is  0.6626710231962477\n",
      "Loss decreases to  0.9888957985244827\n",
      "The norm of grad vector is  0.6618268772002174\n",
      "Loss decreases to  0.9884580599156725\n",
      "The norm of grad vector is  0.6609949591366437\n",
      "Loss decreases to  0.9880214174642739\n",
      "The norm of grad vector is  0.660175011107545\n",
      "Loss decreases to  0.9875858540802485\n",
      "The norm of grad vector is  0.6593667803540347\n",
      "Loss decreases to  0.9871513530634752\n",
      "The norm of grad vector is  0.6585700191686771\n",
      "Loss decreases to  0.9867178980946782\n",
      "The norm of grad vector is  0.6577844848087402\n",
      "Loss decreases to  0.9862854732265698\n",
      "The norm of grad vector is  0.657009939410361\n",
      "Loss decreases to  0.9858540628751956\n",
      "The norm of grad vector is  0.6562461499036343\n",
      "Loss decreases to  0.9854236518114853\n",
      "The norm of grad vector is  0.6554928879286431\n",
      "Loss decreases to  0.9849942251529978\n",
      "The norm of grad vector is  0.6547499297524423\n",
      "Loss decreases to  0.9845657683558564\n",
      "The norm of grad vector is  0.6540170561870047\n",
      "Loss decreases to  0.9841382672068778\n",
      "The norm of grad vector is  0.653294052508143\n",
      "Loss decreases to  0.9837117078158784\n",
      "The norm of grad vector is  0.6525807083754122\n",
      "Loss decreases to  0.9832860766081624\n",
      "The norm of grad vector is  0.6518768177530049\n",
      "Loss decreases to  0.9828613603171853\n",
      "The norm of grad vector is  0.6511821788316403\n",
      "Loss decreases to  0.9824375459773862\n",
      "The norm of grad vector is  0.6504965939514614\n",
      "Loss decreases to  0.9820146209171884\n",
      "The norm of grad vector is  0.6498198695259362\n",
      "Loss decreases to  0.9815925727521635\n",
      "The norm of grad vector is  0.6491518159667723\n",
      "Loss decreases to  0.9811713893783517\n",
      "The norm of grad vector is  0.648492247609847\n",
      "Loss decreases to  0.9807510589657423\n",
      "The norm of grad vector is  0.6478409826421515\n",
      "Loss decreases to  0.9803315699519021\n",
      "The norm of grad vector is  0.6471978430297569\n",
      "Loss decreases to  0.9799129110357515\n",
      "The norm of grad vector is  0.6465626544467951\n",
      "Loss decreases to  0.9794950711714906\n",
      "The norm of grad vector is  0.6459352462054615\n",
      "Loss decreases to  0.9790780395626619\n",
      "The norm of grad vector is  0.6453154511870326\n",
      "Loss decreases to  0.9786618056563524\n",
      "The norm of grad vector is  0.6447031057738986\n",
      "Loss decreases to  0.9782463591375316\n",
      "The norm of grad vector is  0.6440980497826122\n",
      "Loss decreases to  0.9778316899235205\n",
      "The norm of grad vector is  0.6435001263979425\n",
      "Loss decreases to  0.9774177881585909\n",
      "The norm of grad vector is  0.6429091821079426\n",
      "Loss decreases to  0.9770046442086864\n",
      "The norm of grad vector is  0.6423250666400113\n",
      "Loss decreases to  0.9765922486562715\n",
      "The norm of grad vector is  0.6417476328979594\n",
      "Loss decreases to  0.9761805922952973\n",
      "The norm of grad vector is  0.6411767369000622\n",
      "Loss decreases to  0.9757696661262856\n",
      "The norm of grad vector is  0.6406122377181056\n",
      "Loss decreases to  0.9753594613515256\n",
      "The norm of grad vector is  0.6400539974174057\n",
      "Loss decreases to  0.9749499693703866\n",
      "The norm of grad vector is  0.6395018809978077\n",
      "Loss decreases to  0.974541181774735\n",
      "The norm of grad vector is  0.6389557563356478\n",
      "Loss decreases to  0.9741330903444596\n",
      "The norm of grad vector is  0.6384154941266775\n",
      "Loss decreases to  0.9737256870431047\n",
      "The norm of grad vector is  0.6378809678299392\n",
      "Loss decreases to  0.9733189640135985\n",
      "The norm of grad vector is  0.6373520536125818\n",
      "Loss decreases to  0.9729129135740857\n",
      "The norm of grad vector is  0.6368286302956193\n",
      "Loss decreases to  0.972507528213855\n",
      "The norm of grad vector is  0.6363105793006086\n",
      "Loss decreases to  0.9721028005893629\n",
      "The norm of grad vector is  0.6357977845972496\n",
      "Loss decreases to  0.9716987235203488\n",
      "The norm of grad vector is  0.6352901326518922\n",
      "Loss decreases to  0.97129528998604\n",
      "The norm of grad vector is  0.6347875123769441\n",
      "Loss decreases to  0.9708924931214469\n",
      "The norm of grad vector is  0.634289815081166\n",
      "Loss decreases to  0.9704903262137438\n",
      "The norm of grad vector is  0.6337969344208506\n",
      "Loss decreases to  0.9700887826987321\n",
      "The norm of grad vector is  0.6333087663518671\n",
      "Loss decreases to  0.96968785615739\n",
      "The norm of grad vector is  0.6328252090825692\n",
      "Loss decreases to  0.9692875403124963\n",
      "The norm of grad vector is  0.632346163027552\n",
      "Loss decreases to  0.9688878290253401\n",
      "The norm of grad vector is  0.6318715307622484\n",
      "Loss decreases to  0.9684887162925006\n",
      "The norm of grad vector is  0.631401216978352\n",
      "Loss decreases to  0.9680901962427064\n",
      "The norm of grad vector is  0.6309351284400638\n",
      "Loss decreases to  0.9676922631337658\n",
      "The norm of grad vector is  0.6304731739411408\n",
      "Loss decreases to  0.9672949113495671\n",
      "The norm of grad vector is  0.6300152642627467\n",
      "Loss decreases to  0.9668981353971534\n",
      "The norm of grad vector is  0.6295613121320852\n",
      "Loss decreases to  0.9665019299038616\n",
      "The norm of grad vector is  0.6291112321818125\n",
      "Loss decreases to  0.9661062896145284\n",
      "The norm of grad vector is  0.62866494091021\n",
      "Loss decreases to  0.9657112093887614\n",
      "The norm of grad vector is  0.6282223566421178\n",
      "Loss decreases to  0.9653166841982767\n",
      "The norm of grad vector is  0.6277833994906024\n",
      "Loss decreases to  0.964922709124294\n",
      "The norm of grad vector is  0.6273479913193648\n",
      "Loss decreases to  0.9645292793549949\n",
      "The norm of grad vector is  0.6269160557058665\n",
      "Loss decreases to  0.964136390183041\n",
      "The norm of grad vector is  0.6264875179051654\n",
      "Loss decreases to  0.9637440370031473\n",
      "The norm of grad vector is  0.6260623048144535\n",
      "Loss decreases to  0.963352215309713\n",
      "The norm of grad vector is  0.6256403449382814\n",
      "Loss decreases to  0.9629609206945108\n",
      "The norm of grad vector is  0.6252215683544633\n",
      "Loss decreases to  0.9625701488444236\n",
      "The norm of grad vector is  0.6248059066806493\n",
      "Loss decreases to  0.9621798955392395\n",
      "The norm of grad vector is  0.6243932930415528\n",
      "Loss decreases to  0.9617901566494953\n",
      "The norm of grad vector is  0.6239836620368273\n",
      "Loss decreases to  0.9614009281343712\n",
      "The norm of grad vector is  0.6235769497095761\n",
      "Loss decreases to  0.9610122060396333\n",
      "The norm of grad vector is  0.623173093515489\n",
      "Loss decreases to  0.9606239864956274\n",
      "The norm of grad vector is  0.6227720322925954\n",
      "Loss decreases to  0.960236265715314\n",
      "The norm of grad vector is  0.6223737062316193\n",
      "Loss decreases to  0.9598490399923525\n",
      "The norm of grad vector is  0.6219780568469312\n",
      "Loss decreases to  0.9594623056992329\n",
      "The norm of grad vector is  0.6215850269480835\n",
      "Loss decreases to  0.9590760592854428\n",
      "The norm of grad vector is  0.6211945606119211\n",
      "Loss decreases to  0.9586902972756849\n",
      "The norm of grad vector is  0.6208066031552549\n",
      "Loss decreases to  0.9583050162681334\n",
      "The norm of grad vector is  0.6204211011080933\n",
      "Loss decreases to  0.9579202129327273\n",
      "The norm of grad vector is  0.620038002187414\n",
      "Loss decreases to  0.9575358840095108\n",
      "The norm of grad vector is  0.619657255271475\n",
      "Loss decreases to  0.9571520263070058\n",
      "The norm of grad vector is  0.6192788103746495\n",
      "Loss decreases to  0.9567686367006251\n",
      "The norm of grad vector is  0.6189026186227791\n",
      "Loss decreases to  0.9563857121311221\n",
      "The norm of grad vector is  0.61852863222903\n",
      "Loss decreases to  0.9560032496030771\n",
      "The norm of grad vector is  0.6181568044702513\n",
      "Loss decreases to  0.9556212461834184\n",
      "The norm of grad vector is  0.6177870896638209\n",
      "Loss decreases to  0.9552396989999766\n",
      "The norm of grad vector is  0.6174194431449688\n",
      "Loss decreases to  0.9548586052400758\n",
      "The norm of grad vector is  0.6170538212445744\n",
      "Loss decreases to  0.954477962149154\n",
      "The norm of grad vector is  0.6166901812674245\n",
      "Loss decreases to  0.9540977670294171\n",
      "The norm of grad vector is  0.6163284814709246\n",
      "Loss decreases to  0.9537180172385253\n",
      "The norm of grad vector is  0.6159686810442553\n",
      "Loss decreases to  0.9533387101883084\n",
      "The norm of grad vector is  0.6156107400879662\n",
      "Loss decreases to  0.9529598433435115\n",
      "The norm of grad vector is  0.6152546195939969\n",
      "Loss decreases to  0.9525814142205706\n",
      "The norm of grad vector is  0.6149002814261185\n",
      "Loss decreases to  0.9522034203864164\n",
      "The norm of grad vector is  0.6145476883007875\n",
      "Loss decreases to  0.9518258594573038\n",
      "The norm of grad vector is  0.6141968037684041\n",
      "Loss decreases to  0.9514487290976729\n",
      "The norm of grad vector is  0.6138475921949649\n",
      "Loss decreases to  0.951072027019033\n",
      "The norm of grad vector is  0.613500018744108\n",
      "Loss decreases to  0.9506957509788743\n",
      "The norm of grad vector is  0.6131540493595349\n",
      "Loss decreases to  0.950319898779603\n",
      "The norm of grad vector is  0.612809650747809\n",
      "Loss decreases to  0.9499444682675042\n",
      "The norm of grad vector is  0.61246679036152\n",
      "Loss decreases to  0.9495694573317262\n",
      "The norm of grad vector is  0.6121254363828048\n",
      "Loss decreases to  0.949194863903291\n",
      "The norm of grad vector is  0.611785557707226\n",
      "Loss decreases to  0.9488206859541229\n",
      "The norm of grad vector is  0.6114471239279872\n",
      "Loss decreases to  0.9484469214961079\n",
      "The norm of grad vector is  0.6111101053204948\n",
      "Loss decreases to  0.9480735685801671\n",
      "The norm of grad vector is  0.6107744728272463\n",
      "Loss decreases to  0.9477006252953541\n",
      "The norm of grad vector is  0.6104401980430428\n",
      "Loss decreases to  0.9473280897679781\n",
      "The norm of grad vector is  0.6101072532005232\n",
      "Loss decreases to  0.9469559601607387\n",
      "The norm of grad vector is  0.6097756111560072\n",
      "Loss decreases to  0.9465842346718899\n",
      "The norm of grad vector is  0.6094452453756453\n",
      "Loss decreases to  0.9462129115344143\n",
      "The norm of grad vector is  0.609116129921866\n",
      "Loss decreases to  0.9458419890152273\n",
      "The norm of grad vector is  0.6087882394401196\n",
      "Loss decreases to  0.9454714654143892\n",
      "The norm of grad vector is  0.6084615491459056\n",
      "Loss decreases to  0.9451013390643418\n",
      "The norm of grad vector is  0.6081360348120839\n",
      "Loss decreases to  0.944731608329163\n",
      "The norm of grad vector is  0.6078116727564593\n",
      "Loss decreases to  0.9443622716038348\n",
      "The norm of grad vector is  0.6074884398296383\n",
      "Loss decreases to  0.9439933273135319\n",
      "The norm of grad vector is  0.6071663134031482\n",
      "Loss decreases to  0.943624773912925\n",
      "The norm of grad vector is  0.6068452713578164\n",
      "Loss decreases to  0.9432566098855026\n",
      "The norm of grad vector is  0.6065252920724009\n",
      "Loss decreases to  0.9428888337429037\n",
      "The norm of grad vector is  0.606206354412472\n",
      "Loss decreases to  0.9425214440242722\n",
      "The norm of grad vector is  0.6058884377195344\n",
      "Loss decreases to  0.9421544392956226\n",
      "The norm of grad vector is  0.6055715218003902\n",
      "Loss decreases to  0.9417878181492192\n",
      "The norm of grad vector is  0.60525558691673\n",
      "Loss decreases to  0.9414215792029753\n",
      "The norm of grad vector is  0.6049406137749573\n",
      "Loss decreases to  0.9410557210998605\n",
      "The norm of grad vector is  0.6046265835162337\n",
      "Loss decreases to  0.9406902425073229\n",
      "The norm of grad vector is  0.604313477706741\n",
      "Loss decreases to  0.9403251421167299\n",
      "The norm of grad vector is  0.6040012783281615\n",
      "Loss decreases to  0.9399604186428153\n",
      "The norm of grad vector is  0.6036899677683641\n",
      "Loss decreases to  0.9395960708231411\n",
      "The norm of grad vector is  0.6033795288122965\n",
      "Loss decreases to  0.9392320974175767\n",
      "The norm of grad vector is  0.6030699446330791\n",
      "Loss decreases to  0.9388684972077821\n",
      "The norm of grad vector is  0.6027611987832933\n",
      "Loss decreases to  0.9385052689967095\n",
      "The norm of grad vector is  0.6024532751864637\n",
      "Loss decreases to  0.9381424116081143\n",
      "The norm of grad vector is  0.6021461581287288\n",
      "Loss decreases to  0.9377799238860768\n",
      "The norm of grad vector is  0.6018398322506927\n",
      "Loss decreases to  0.9374178046945363\n",
      "The norm of grad vector is  0.6015342825394622\n",
      "Loss decreases to  0.9370560529168349\n",
      "The norm of grad vector is  0.6012294943208532\n",
      "Loss decreases to  0.9366946674552734\n",
      "The norm of grad vector is  0.6009254532517772\n",
      "Loss decreases to  0.9363336472306751\n",
      "The norm of grad vector is  0.600622145312791\n",
      "Loss decreases to  0.9359729911819629\n",
      "The norm of grad vector is  0.6003195568008147\n",
      "Loss decreases to  0.9356126982657439\n",
      "The norm of grad vector is  0.6000176743220094\n",
      "Loss decreases to  0.9352527674559037\n",
      "The norm of grad vector is  0.5997164847848142\n",
      "Loss decreases to  0.9348931977432121\n",
      "The norm of grad vector is  0.5994159753931374\n",
      "Loss decreases to  0.9345339881349358\n",
      "The norm of grad vector is  0.5991161336397004\n",
      "Loss decreases to  0.9341751376544613\n",
      "The norm of grad vector is  0.5988169472995272\n",
      "Loss decreases to  0.933816645340926\n",
      "The norm of grad vector is  0.5985184044235807\n",
      "Loss decreases to  0.9334585102488584\n",
      "The norm of grad vector is  0.5982204933325411\n",
      "Loss decreases to  0.9331007314478261\n",
      "The norm of grad vector is  0.5979232026107206\n",
      "Loss decreases to  0.9327433080220934\n",
      "The norm of grad vector is  0.5976265211001165\n",
      "Loss decreases to  0.9323862390702838\n",
      "The norm of grad vector is  0.5973304378945964\n",
      "Loss decreases to  0.9320295237050545\n",
      "The norm of grad vector is  0.5970349423342101\n",
      "Loss decreases to  0.9316731610527738\n",
      "The norm of grad vector is  0.5967400239996341\n",
      "Loss decreases to  0.9313171502532117\n",
      "The norm of grad vector is  0.596445672706735\n",
      "Loss decreases to  0.9309614904592307\n",
      "The norm of grad vector is  0.5961518785012578\n",
      "Loss decreases to  0.9306061808364918\n",
      "The norm of grad vector is  0.5958586316536313\n",
      "Loss decreases to  0.9302512205631591\n",
      "The norm of grad vector is  0.5955659226538897\n",
      "Loss decreases to  0.929896608829618\n",
      "The norm of grad vector is  0.5952737422067106\n",
      "Loss decreases to  0.929542344838196\n",
      "The norm of grad vector is  0.5949820812265584\n",
      "Loss decreases to  0.9291884278028918\n",
      "The norm of grad vector is  0.5946909308329434\n",
      "Loss decreases to  0.9288348569491098\n",
      "The norm of grad vector is  0.5944002823457827\n",
      "Loss decreases to  0.9284816315134014\n",
      "The norm of grad vector is  0.5941101272808654\n",
      "Loss decreases to  0.9281287507432114\n",
      "The norm of grad vector is  0.5938204573454217\n",
      "Loss decreases to  0.9277762138966322\n",
      "The norm of grad vector is  0.5935312644337893\n",
      "Loss decreases to  0.9274240202421611\n",
      "The norm of grad vector is  0.5932425406231783\n",
      "Loss decreases to  0.9270721690584649\n",
      "The norm of grad vector is  0.5929542781695303\n",
      "Loss decreases to  0.926720659634151\n",
      "The norm of grad vector is  0.5926664695034723\n",
      "Loss decreases to  0.9263694912675402\n",
      "The norm of grad vector is  0.5923791072263598\n",
      "Loss decreases to  0.9260186632664488\n",
      "The norm of grad vector is  0.592092184106408\n",
      "Loss decreases to  0.9256681749479734\n",
      "The norm of grad vector is  0.5918056930749136\n",
      "Loss decreases to  0.9253180256382819\n",
      "The norm of grad vector is  0.5915196272225578\n",
      "Loss decreases to  0.9249682146724074\n",
      "The norm of grad vector is  0.5912339797957945\n",
      "Loss decreases to  0.9246187413940505\n",
      "The norm of grad vector is  0.5909487441933211\n",
      "Loss decreases to  0.9242696051553814\n",
      "The norm of grad vector is  0.5906639139626242\n",
      "Loss decreases to  0.9239208053168514\n",
      "The norm of grad vector is  0.5903794827966077\n",
      "Loss decreases to  0.9235723412470056\n",
      "The norm of grad vector is  0.5900954445302955\n",
      "Loss decreases to  0.923224212322301\n",
      "The norm of grad vector is  0.5898117931376073\n",
      "Loss decreases to  0.9228764179269289\n",
      "The norm of grad vector is  0.5895285227282087\n",
      "Loss decreases to  0.9225289574526396\n",
      "The norm of grad vector is  0.5892456275444303\n",
      "Loss decreases to  0.9221818302985771\n",
      "The norm of grad vector is  0.5889631019582602\n",
      "Loss decreases to  0.921835035871109\n",
      "The norm of grad vector is  0.5886809404684001\n",
      "Loss decreases to  0.9214885735836668\n",
      "The norm of grad vector is  0.5883991376973893\n",
      "Loss decreases to  0.9211424428565878\n",
      "The norm of grad vector is  0.5881176883887946\n",
      "Loss decreases to  0.92079664311696\n",
      "The norm of grad vector is  0.5878365874044629\n",
      "Loss decreases to  0.9204511737984722\n",
      "The norm of grad vector is  0.5875558297218353\n",
      "Loss decreases to  0.9201060343412666\n",
      "The norm of grad vector is  0.5872754104313249\n",
      "Loss decreases to  0.9197612241917943\n",
      "The norm of grad vector is  0.5869953247337472\n",
      "Loss decreases to  0.9194167428026755\n",
      "The norm of grad vector is  0.5867155679378174\n",
      "Loss decreases to  0.9190725896325616\n",
      "The norm of grad vector is  0.5864361354576954\n",
      "Loss decreases to  0.9187287641460016\n",
      "The norm of grad vector is  0.5861570228105953\n",
      "Loss decreases to  0.9183852658133119\n",
      "The norm of grad vector is  0.5858782256144395\n",
      "Loss decreases to  0.9180420941104468\n",
      "The norm of grad vector is  0.5855997395855751\n",
      "Loss decreases to  0.9176992485188749\n",
      "The norm of grad vector is  0.5853215605365342\n",
      "Loss decreases to  0.9173567285254567\n",
      "The norm of grad vector is  0.5850436843738501\n",
      "Loss decreases to  0.9170145336223248\n",
      "The norm of grad vector is  0.5847661070959205\n",
      "Loss decreases to  0.9166726633067682\n",
      "The norm of grad vector is  0.5844888247909197\n",
      "Loss decreases to  0.9163311170811189\n",
      "The norm of grad vector is  0.5842118336347575\n",
      "Loss decreases to  0.9159898944526396\n",
      "The norm of grad vector is  0.5839351298890864\n",
      "Loss decreases to  0.915648994933417\n",
      "The norm of grad vector is  0.5836587098993512\n",
      "Loss decreases to  0.9153084180402535\n",
      "The norm of grad vector is  0.5833825700928832\n",
      "Loss decreases to  0.9149681632945663\n",
      "The norm of grad vector is  0.5831067069770413\n",
      "Loss decreases to  0.9146282302222843\n",
      "The norm of grad vector is  0.5828311171373887\n",
      "Loss decreases to  0.9142886183537501\n",
      "The norm of grad vector is  0.582555797235917\n",
      "Loss decreases to  0.9139493272236233\n",
      "The norm of grad vector is  0.5822807440093081\n",
      "Loss decreases to  0.9136103563707869\n",
      "The norm of grad vector is  0.5820059542672331\n",
      "Loss decreases to  0.913271705338255\n",
      "The norm of grad vector is  0.5817314248906966\n",
      "Loss decreases to  0.9129333736730824\n",
      "The norm of grad vector is  0.5814571528304083\n",
      "Loss decreases to  0.9125953609262766\n",
      "The norm of grad vector is  0.5811831351052035\n",
      "Loss decreases to  0.9122576666527136\n",
      "The norm of grad vector is  0.5809093688004883\n",
      "Loss decreases to  0.9119202904110522\n",
      "The norm of grad vector is  0.5806358510667283\n",
      "Loss decreases to  0.911583231763654\n",
      "The norm of grad vector is  0.5803625791179683\n",
      "Loss decreases to  0.9112464902765014\n",
      "The norm of grad vector is  0.5800895502303826\n",
      "Loss decreases to  0.9109100655191218\n",
      "The norm of grad vector is  0.5798167617408649\n",
      "Loss decreases to  0.9105739570645088\n",
      "The norm of grad vector is  0.5795442110456454\n",
      "Loss decreases to  0.9102381644890505\n",
      "The norm of grad vector is  0.5792718955989395\n",
      "Loss decreases to  0.909902687372454\n",
      "The norm of grad vector is  0.57899981291163\n",
      "Loss decreases to  0.9095675252976768\n",
      "The norm of grad vector is  0.578727960549976\n",
      "Loss decreases to  0.909232677850855\n",
      "The norm of grad vector is  0.5784563361343534\n",
      "Loss decreases to  0.9088981446212379\n",
      "The norm of grad vector is  0.5781849373380231\n",
      "Loss decreases to  0.9085639252011192\n",
      "The norm of grad vector is  0.5779137618859264\n",
      "Loss decreases to  0.9082300191857743\n",
      "The norm of grad vector is  0.57764280755351\n",
      "Loss decreases to  0.9078964261733963\n",
      "The norm of grad vector is  0.5773720721655756\n",
      "Loss decreases to  0.9075631457650344\n",
      "The norm of grad vector is  0.5771015535951567\n",
      "Loss decreases to  0.9072301775645335\n",
      "The norm of grad vector is  0.5768312497624196\n",
      "Loss decreases to  0.9068975211784751\n",
      "The norm of grad vector is  0.5765611586335926\n",
      "Loss decreases to  0.9065651762161199\n",
      "The norm of grad vector is  0.5762912782199143\n",
      "Loss decreases to  0.9062331422893515\n",
      "The norm of grad vector is  0.5760216065766114\n",
      "Loss decreases to  0.9059014190126224\n",
      "The norm of grad vector is  0.5757521418018968\n",
      "Loss decreases to  0.9055700060028987\n",
      "The norm of grad vector is  0.5754828820359896\n",
      "Loss decreases to  0.9052389028796085\n",
      "The norm of grad vector is  0.5752138254601595\n",
      "Loss decreases to  0.9049081092645921\n",
      "The norm of grad vector is  0.5749449702957923\n",
      "Loss decreases to  0.904577624782049\n",
      "The norm of grad vector is  0.5746763148034761\n",
      "Loss decreases to  0.9042474490584911\n",
      "The norm of grad vector is  0.5744078572821087\n",
      "Loss decreases to  0.9039175817226957\n",
      "The norm of grad vector is  0.5741395960680252\n",
      "Loss decreases to  0.9035880224056568\n",
      "The norm of grad vector is  0.5738715295341461\n",
      "Loss decreases to  0.9032587707405407\n",
      "The norm of grad vector is  0.5736036560891427\n",
      "Loss decreases to  0.9029298263626416\n",
      "The norm of grad vector is  0.5733359741766244\n",
      "Loss decreases to  0.9026011889093384\n",
      "The norm of grad vector is  0.5730684822743429\n",
      "Loss decreases to  0.9022728580200509\n",
      "The norm of grad vector is  0.5728011788934138\n",
      "Loss decreases to  0.9019448333362001\n",
      "The norm of grad vector is  0.5725340625775572\n",
      "Loss decreases to  0.9016171145011661\n",
      "The norm of grad vector is  0.5722671319023562\n",
      "Loss decreases to  0.9012897011602503\n",
      "The norm of grad vector is  0.5720003854745291\n",
      "Loss decreases to  0.9009625929606355\n",
      "The norm of grad vector is  0.5717338219312219\n",
      "Loss decreases to  0.9006357895513473\n",
      "The norm of grad vector is  0.5714674399393155\n",
      "Loss decreases to  0.9003092905832202\n",
      "The norm of grad vector is  0.5712012381947474\n",
      "Loss decreases to  0.8999830957088584\n",
      "The norm of grad vector is  0.5709352154218504\n",
      "Loss decreases to  0.8996572045826023\n",
      "The norm of grad vector is  0.5706693703727063\n",
      "Loss decreases to  0.8993316168604941\n",
      "The norm of grad vector is  0.5704037018265131\n",
      "Loss decreases to  0.8990063322002432\n",
      "The norm of grad vector is  0.5701382085889682\n",
      "Loss decreases to  0.8986813502611948\n",
      "The norm of grad vector is  0.5698728894916643\n",
      "Loss decreases to  0.8983566707042968\n",
      "The norm of grad vector is  0.5696077433915\n",
      "Loss decreases to  0.898032293192069\n",
      "The norm of grad vector is  0.5693427691701033\n",
      "Loss decreases to  0.8977082173885714\n",
      "The norm of grad vector is  0.5690779657332676\n",
      "Loss decreases to  0.8973844429593771\n",
      "The norm of grad vector is  0.5688133320104037\n",
      "Loss decreases to  0.8970609695715395\n",
      "The norm of grad vector is  0.5685488669539986\n",
      "Loss decreases to  0.8967377968935665\n",
      "The norm of grad vector is  0.5682845695390922\n",
      "Loss decreases to  0.8964149245953912\n",
      "The norm of grad vector is  0.5680204387627634\n",
      "Loss decreases to  0.8960923523483456\n",
      "The norm of grad vector is  0.567756473643627\n",
      "Loss decreases to  0.8957700798251336\n",
      "The norm of grad vector is  0.5674926732213449\n",
      "Loss decreases to  0.8954481066998043\n",
      "The norm of grad vector is  0.5672290365561451\n",
      "Loss decreases to  0.8951264326477282\n",
      "The norm of grad vector is  0.5669655627283547\n",
      "Loss decreases to  0.8948050573455714\n",
      "The norm of grad vector is  0.5667022508379409\n",
      "Loss decreases to  0.8944839804712708\n",
      "The norm of grad vector is  0.5664391000040649\n",
      "Loss decreases to  0.8941632017040123\n",
      "The norm of grad vector is  0.5661761093646439\n",
      "Loss decreases to  0.8938427207242058\n",
      "The norm of grad vector is  0.5659132780759247\n",
      "Loss decreases to  0.8935225372134636\n",
      "The norm of grad vector is  0.565650605312065\n",
      "Loss decreases to  0.893202650854578\n",
      "The norm of grad vector is  0.5653880902647271\n",
      "Loss decreases to  0.8928830613315\n",
      "The norm of grad vector is  0.5651257321426786\n",
      "Loss decreases to  0.8925637683293178\n",
      "The norm of grad vector is  0.5648635301714021\n",
      "Loss decreases to  0.8922447715342374\n",
      "The norm of grad vector is  0.5646014835927166\n",
      "Loss decreases to  0.8919260706335603\n",
      "The norm of grad vector is  0.5643395916644028\n",
      "Loss decreases to  0.8916076653156668\n",
      "The norm of grad vector is  0.5640778536598423\n",
      "Loss decreases to  0.8912895552699936\n",
      "The norm of grad vector is  0.5638162688676601\n",
      "Loss decreases to  0.8909717401870181\n",
      "The norm of grad vector is  0.5635548365913792\n",
      "Loss decreases to  0.8906542197582373\n",
      "The norm of grad vector is  0.5632935561490805\n",
      "Loss decreases to  0.8903369936761522\n",
      "The norm of grad vector is  0.563032426873072\n",
      "Loss decreases to  0.8900200616342487\n",
      "The norm of grad vector is  0.5627714481095635\n",
      "Loss decreases to  0.8897034233269823\n",
      "The norm of grad vector is  0.5625106192183525\n",
      "Loss decreases to  0.8893870784497586\n",
      "The norm of grad vector is  0.5622499395725122\n",
      "Loss decreases to  0.8890710266989207\n",
      "The norm of grad vector is  0.5619894085580912\n",
      "Loss decreases to  0.88875526777173\n",
      "The norm of grad vector is  0.5617290255738174\n",
      "Loss decreases to  0.8884398013663516\n",
      "The norm of grad vector is  0.5614687900308094\n",
      "Loss decreases to  0.8881246271818398\n",
      "The norm of grad vector is  0.5612087013522944\n",
      "Loss decreases to  0.8878097449181236\n",
      "The norm of grad vector is  0.5609487589733322\n",
      "Loss decreases to  0.8874951542759907\n",
      "The norm of grad vector is  0.5606889623405475\n",
      "Loss decreases to  0.8871808549570734\n",
      "The norm of grad vector is  0.560429310911865\n",
      "Loss decreases to  0.8868668466638366\n",
      "The norm of grad vector is  0.5601698041562522\n",
      "Loss decreases to  0.8865531290995622\n",
      "The norm of grad vector is  0.5599104415534696\n",
      "Loss decreases to  0.8862397019683368\n",
      "The norm of grad vector is  0.5596512225938238\n",
      "Loss decreases to  0.8859265649750389\n",
      "The norm of grad vector is  0.5593921467779276\n",
      "Loss decreases to  0.8856137178253255\n",
      "The norm of grad vector is  0.5591332136164661\n",
      "Loss decreases to  0.885301160225621\n",
      "The norm of grad vector is  0.558874422629967\n",
      "Loss decreases to  0.8849888918831034\n",
      "The norm of grad vector is  0.5586157733485773\n",
      "Loss decreases to  0.884676912505694\n",
      "The norm of grad vector is  0.5583572653118434\n",
      "Loss decreases to  0.8843652218020459\n",
      "The norm of grad vector is  0.558098898068498\n",
      "Loss decreases to  0.8840538194815312\n",
      "The norm of grad vector is  0.5578406711762518\n",
      "Loss decreases to  0.8837427052542317\n",
      "The norm of grad vector is  0.5575825842015872\n",
      "Loss decreases to  0.8834318788309268\n",
      "The norm of grad vector is  0.5573246367195613\n",
      "Loss decreases to  0.8831213399230845\n",
      "The norm of grad vector is  0.5570668283136091\n",
      "Loss decreases to  0.8828110882428495\n",
      "The norm of grad vector is  0.5568091585753532\n",
      "Loss decreases to  0.882501123503034\n",
      "The norm of grad vector is  0.5565516271044187\n",
      "Loss decreases to  0.8821914454171085\n",
      "The norm of grad vector is  0.5562942335082504\n",
      "Loss decreases to  0.8818820536991907\n",
      "The norm of grad vector is  0.5560369774019344\n",
      "Loss decreases to  0.8815729480640376\n",
      "The norm of grad vector is  0.5557798584080254\n",
      "Loss decreases to  0.8812641282270351\n",
      "The norm of grad vector is  0.5555228761563769\n",
      "Loss decreases to  0.8809555939041904\n",
      "The norm of grad vector is  0.5552660302839746\n",
      "Loss decreases to  0.8806473448121219\n",
      "The norm of grad vector is  0.5550093204347748\n",
      "Loss decreases to  0.8803393806680513\n",
      "The norm of grad vector is  0.5547527462595457\n",
      "Loss decreases to  0.8800317011897956\n",
      "The norm of grad vector is  0.5544963074157127\n",
      "Loss decreases to  0.8797243060957588\n",
      "The norm of grad vector is  0.5542400035672074\n",
      "Loss decreases to  0.879417195104922\n",
      "The norm of grad vector is  0.5539838343843189\n",
      "Loss decreases to  0.8791103679368399\n",
      "The norm of grad vector is  0.5537277995435507\n",
      "Loss decreases to  0.8788038243116283\n",
      "The norm of grad vector is  0.5534718987274776\n",
      "Loss decreases to  0.8784975639499605\n",
      "The norm of grad vector is  0.5532161316246106\n",
      "Loss decreases to  0.8781915865730568\n",
      "The norm of grad vector is  0.5529604979292586\n",
      "Loss decreases to  0.8778858919026808\n",
      "The norm of grad vector is  0.5527049973414004\n",
      "Loss decreases to  0.8775804796611294\n",
      "The norm of grad vector is  0.5524496295665525\n",
      "Loss decreases to  0.8772753495712275\n",
      "The norm of grad vector is  0.5521943943156459\n",
      "Loss decreases to  0.8769705013563217\n",
      "The norm of grad vector is  0.5519392913049018\n",
      "Loss decreases to  0.8766659347402722\n",
      "The norm of grad vector is  0.5516843202557119\n",
      "Loss decreases to  0.8763616494474482\n",
      "The norm of grad vector is  0.5514294808945216\n",
      "Loss decreases to  0.87605764520272\n",
      "The norm of grad vector is  0.5511747729527134\n",
      "Loss decreases to  0.8757539217314559\n",
      "The norm of grad vector is  0.5509201961664983\n",
      "Loss decreases to  0.875450478759512\n",
      "The norm of grad vector is  0.5506657502768023\n",
      "Loss decreases to  0.8751473160132295\n",
      "The norm of grad vector is  0.5504114350291625\n",
      "Loss decreases to  0.8748444332194286\n",
      "The norm of grad vector is  0.5501572501736203\n",
      "Loss decreases to  0.8745418301054018\n",
      "The norm of grad vector is  0.5499031954646209\n",
      "Loss decreases to  0.8742395063989093\n",
      "The norm of grad vector is  0.5496492706609126\n",
      "Loss decreases to  0.8739374618281742\n",
      "The norm of grad vector is  0.5493954755254494\n",
      "Loss decreases to  0.8736356961218763\n",
      "The norm of grad vector is  0.5491418098252958\n",
      "Loss decreases to  0.8733342090091467\n",
      "The norm of grad vector is  0.548888273331533\n",
      "Loss decreases to  0.8730330002195646\n",
      "The norm of grad vector is  0.5486348658191688\n",
      "Loss decreases to  0.8727320694831513\n",
      "The norm of grad vector is  0.5483815870670479\n",
      "Loss decreases to  0.8724314165303652\n",
      "The norm of grad vector is  0.5481284368577647\n",
      "Loss decreases to  0.8721310410920976\n",
      "The norm of grad vector is  0.5478754149775804\n",
      "Loss decreases to  0.8718309428996686\n",
      "The norm of grad vector is  0.5476225212163367\n",
      "Loss decreases to  0.8715311216848215\n",
      "The norm of grad vector is  0.547369755367377\n",
      "Loss decreases to  0.87123157717972\n",
      "The norm of grad vector is  0.5471171172274673\n",
      "Loss decreases to  0.8709323091169421\n",
      "The norm of grad vector is  0.5468646065967163\n",
      "Loss decreases to  0.8706333172294779\n",
      "The norm of grad vector is  0.546612223278503\n",
      "Loss decreases to  0.8703346012507244\n",
      "The norm of grad vector is  0.5463599670794005\n",
      "Loss decreases to  0.8700361609144815\n",
      "The norm of grad vector is  0.5461078378091041\n",
      "Loss decreases to  0.8697379959549483\n",
      "The norm of grad vector is  0.5458558352803609\n",
      "Loss decreases to  0.8694401061067197\n",
      "The norm of grad vector is  0.5456039593089006\n",
      "Loss decreases to  0.8691424911047828\n",
      "The norm of grad vector is  0.5453522097133693\n",
      "Loss decreases to  0.8688451506845121\n",
      "The norm of grad vector is  0.5451005863152613\n",
      "Loss decreases to  0.8685480845816668\n",
      "The norm of grad vector is  0.5448490889388561\n",
      "Loss decreases to  0.8682512925323884\n",
      "The norm of grad vector is  0.5445977174111558\n",
      "Loss decreases to  0.8679547742731941\n",
      "The norm of grad vector is  0.5443464715618228\n",
      "Loss decreases to  0.8676585295409773\n",
      "The norm of grad vector is  0.5440953512231194\n",
      "Loss decreases to  0.8673625580730018\n",
      "The norm of grad vector is  0.5438443562298505\n",
      "Loss decreases to  0.8670668596068998\n",
      "The norm of grad vector is  0.543593486419305\n",
      "Loss decreases to  0.8667714338806678\n",
      "The norm of grad vector is  0.5433427416312002\n",
      "Loss decreases to  0.8664762806326646\n",
      "The norm of grad vector is  0.5430921217076269\n",
      "Loss decreases to  0.866181399601608\n",
      "The norm of grad vector is  0.542841626492995\n",
      "Loss decreases to  0.8658867905265711\n",
      "The norm of grad vector is  0.5425912558339839\n",
      "Loss decreases to  0.8655924531469807\n",
      "The norm of grad vector is  0.5423410095794878\n",
      "Loss decreases to  0.8652983872026146\n",
      "The norm of grad vector is  0.5420908875805681\n",
      "Loss decreases to  0.8650045924335962\n",
      "The norm of grad vector is  0.541840889690404\n",
      "Loss decreases to  0.8647110685803959\n",
      "The norm of grad vector is  0.5415910157642443\n",
      "Loss decreases to  0.8644178153838246\n",
      "The norm of grad vector is  0.5413412656593617\n",
      "Loss decreases to  0.8641248325850344\n",
      "The norm of grad vector is  0.5410916392350061\n",
      "Loss decreases to  0.8638321199255143\n",
      "The norm of grad vector is  0.540842136352362\n",
      "Loss decreases to  0.8635396771470873\n",
      "The norm of grad vector is  0.5405927568745017\n",
      "Loss decreases to  0.86324750399191\n",
      "The norm of grad vector is  0.5403435006663466\n",
      "Loss decreases to  0.8629556002024679\n",
      "The norm of grad vector is  0.5400943675946233\n",
      "Loss decreases to  0.8626639655215748\n",
      "The norm of grad vector is  0.5398453575278228\n",
      "Loss decreases to  0.8623725996923701\n",
      "The norm of grad vector is  0.5395964703361629\n",
      "Loss decreases to  0.8620815024583168\n",
      "The norm of grad vector is  0.5393477058915475\n",
      "Loss decreases to  0.861790673563198\n",
      "The norm of grad vector is  0.5390990640675292\n",
      "Loss decreases to  0.8615001127511164\n",
      "The norm of grad vector is  0.5388505447392726\n",
      "Loss decreases to  0.8612098197664922\n",
      "The norm of grad vector is  0.5386021477835187\n",
      "Loss decreases to  0.8609197943540592\n",
      "The norm of grad vector is  0.538353873078548\n",
      "Loss decreases to  0.8606300362588657\n",
      "The norm of grad vector is  0.5381057205041467\n",
      "Loss decreases to  0.8603405452262697\n",
      "The norm of grad vector is  0.537857689941574\n",
      "Loss decreases to  0.8600513210019393\n",
      "The norm of grad vector is  0.5376097812735275\n",
      "Loss decreases to  0.8597623633318492\n",
      "The norm of grad vector is  0.5373619943841119\n",
      "Loss decreases to  0.8594736719622798\n",
      "The norm of grad vector is  0.5371143291588073\n",
      "Loss decreases to  0.8591852466398148\n",
      "The norm of grad vector is  0.5368667854844387\n",
      "Loss decreases to  0.8588970871113402\n",
      "The norm of grad vector is  0.5366193632491447\n",
      "Loss decreases to  0.8586091931240419\n",
      "The norm of grad vector is  0.53637206234235\n",
      "Loss decreases to  0.8583215644254044\n",
      "The norm of grad vector is  0.5361248826547358\n",
      "Loss decreases to  0.8580342007632095\n",
      "The norm of grad vector is  0.5358778240782114\n",
      "Loss decreases to  0.8577471018855325\n",
      "The norm of grad vector is  0.5356308865058863\n",
      "Loss decreases to  0.857460267540745\n",
      "The norm of grad vector is  0.5353840698320459\n",
      "Loss decreases to  0.8571736974775083\n",
      "The norm of grad vector is  0.5351373739521224\n",
      "Loss decreases to  0.8568873914447758\n",
      "The norm of grad vector is  0.5348907987626706\n",
      "Loss decreases to  0.8566013491917889\n",
      "The norm of grad vector is  0.5346443441613425\n",
      "Loss decreases to  0.8563155704680777\n",
      "The norm of grad vector is  0.5343980100468645\n",
      "Loss decreases to  0.8560300550234577\n",
      "The norm of grad vector is  0.5341517963190101\n",
      "Loss decreases to  0.8557448026080299\n",
      "The norm of grad vector is  0.5339057028785799\n",
      "Loss decreases to  0.8554598129721773\n",
      "The norm of grad vector is  0.5336597296273776\n",
      "Loss decreases to  0.8551750858665672\n",
      "The norm of grad vector is  0.5334138764681867\n",
      "Loss decreases to  0.8548906210421459\n",
      "The norm of grad vector is  0.5331681433047505\n",
      "Loss decreases to  0.8546064182501398\n",
      "The norm of grad vector is  0.5329225300417497\n",
      "Loss decreases to  0.854322477242053\n",
      "The norm of grad vector is  0.5326770365847818\n",
      "Loss decreases to  0.8540387977696681\n",
      "The norm of grad vector is  0.5324316628403412\n",
      "Loss decreases to  0.8537553795850412\n",
      "The norm of grad vector is  0.5321864087157986\n",
      "Loss decreases to  0.8534722224405049\n",
      "The norm of grad vector is  0.531941274119383\n",
      "Loss decreases to  0.8531893260886638\n",
      "The norm of grad vector is  0.5316962589601605\n",
      "Loss decreases to  0.8529066902823959\n",
      "The norm of grad vector is  0.5314513631480181\n",
      "Loss decreases to  0.8526243147748488\n",
      "The norm of grad vector is  0.5312065865936442\n",
      "Loss decreases to  0.8523421993194419\n",
      "The norm of grad vector is  0.5309619292085113\n",
      "Loss decreases to  0.8520603436698617\n",
      "The norm of grad vector is  0.5307173909048588\n",
      "Loss decreases to  0.8517787475800637\n",
      "The norm of grad vector is  0.5304729715956759\n",
      "Loss decreases to  0.8514974108042693\n",
      "The norm of grad vector is  0.5302286711946858\n",
      "Loss decreases to  0.8512163330969672\n",
      "The norm of grad vector is  0.529984489616328\n",
      "Loss decreases to  0.850935514212909\n",
      "The norm of grad vector is  0.5297404267757443\n",
      "Loss decreases to  0.850654953907111\n",
      "The norm of grad vector is  0.5294964825887628\n",
      "Loss decreases to  0.8503746519348523\n",
      "The norm of grad vector is  0.529252656971882\n",
      "Loss decreases to  0.8500946080516734\n",
      "The norm of grad vector is  0.5290089498422575\n",
      "Loss decreases to  0.8498148220133761\n",
      "The norm of grad vector is  0.5287653611176868\n",
      "Loss decreases to  0.8495352935760223\n",
      "The norm of grad vector is  0.5285218907165944\n",
      "Loss decreases to  0.8492560224959318\n",
      "The norm of grad vector is  0.5282785385580206\n",
      "Loss decreases to  0.8489770085296837\n",
      "The norm of grad vector is  0.5280353045616059\n",
      "Loss decreases to  0.8486982514341145\n",
      "The norm of grad vector is  0.5277921886475784\n",
      "Loss decreases to  0.8484197509663166\n",
      "The norm of grad vector is  0.5275491907367412\n",
      "Loss decreases to  0.848141506883638\n",
      "The norm of grad vector is  0.5273063107504598\n",
      "Loss decreases to  0.8478635189436814\n",
      "The norm of grad vector is  0.5270635486106504\n",
      "Loss decreases to  0.8475857869043039\n",
      "The norm of grad vector is  0.5268209042397671\n",
      "Loss decreases to  0.8473083105236154\n",
      "The norm of grad vector is  0.5265783775607902\n",
      "Loss decreases to  0.8470310895599787\n",
      "The norm of grad vector is  0.5263359684972165\n",
      "Loss decreases to  0.846754123772008\n",
      "The norm of grad vector is  0.5260936769730455\n",
      "Loss decreases to  0.846477412918567\n",
      "The norm of grad vector is  0.5258515029127704\n",
      "Loss decreases to  0.8462009567587718\n",
      "The norm of grad vector is  0.5256094462413667\n",
      "Loss decreases to  0.845924755051986\n",
      "The norm of grad vector is  0.5253675068842824\n",
      "Loss decreases to  0.8456488075578228\n",
      "The norm of grad vector is  0.525125684767427\n",
      "Loss decreases to  0.8453731140361431\n",
      "The norm of grad vector is  0.5248839798171622\n",
      "Loss decreases to  0.8450976742470551\n",
      "The norm of grad vector is  0.5246423919602923\n",
      "Loss decreases to  0.8448224879509133\n",
      "The norm of grad vector is  0.5244009211240535\n",
      "Loss decreases to  0.8445475549083186\n",
      "The norm of grad vector is  0.5241595672361069\n",
      "Loss decreases to  0.8442728748801167\n",
      "The norm of grad vector is  0.5239183302245267\n",
      "Loss decreases to  0.8439984476273975\n",
      "The norm of grad vector is  0.5236772100177937\n",
      "Loss decreases to  0.8437242729114954\n",
      "The norm of grad vector is  0.5234362065447847\n",
      "Loss decreases to  0.8434503504939884\n",
      "The norm of grad vector is  0.5231953197347664\n",
      "Loss decreases to  0.843176680136696\n",
      "The norm of grad vector is  0.5229545495173845\n",
      "Loss decreases to  0.8429032616016812\n",
      "The norm of grad vector is  0.5227138958226577\n",
      "Loss decreases to  0.8426300946512472\n",
      "The norm of grad vector is  0.5224733585809685\n",
      "Loss decreases to  0.8423571790479385\n",
      "The norm of grad vector is  0.5222329377230561\n",
      "Loss decreases to  0.8420845145545401\n",
      "The norm of grad vector is  0.5219926331800093\n",
      "Loss decreases to  0.8418121009340763\n",
      "The norm of grad vector is  0.5217524448832576\n",
      "Loss decreases to  0.8415399379498107\n",
      "The norm of grad vector is  0.5215123727645657\n",
      "Loss decreases to  0.8412680253652457\n",
      "The norm of grad vector is  0.5212724167560256\n",
      "Loss decreases to  0.8409963629441213\n",
      "The norm of grad vector is  0.5210325767900501\n",
      "Loss decreases to  0.8407249504504146\n",
      "The norm of grad vector is  0.5207928527993656\n",
      "Loss decreases to  0.8404537876483417\n",
      "The norm of grad vector is  0.5205532447170059\n",
      "Loss decreases to  0.8401828743023518\n",
      "The norm of grad vector is  0.5203137524763057\n",
      "Loss decreases to  0.839912210177133\n",
      "The norm of grad vector is  0.5200743760108945\n",
      "Loss decreases to  0.8396417950376064\n",
      "The norm of grad vector is  0.5198351152546902\n",
      "Loss decreases to  0.83937162864893\n",
      "The norm of grad vector is  0.5195959701418935\n",
      "Loss decreases to  0.839101710776495\n",
      "The norm of grad vector is  0.5193569406069813\n",
      "Loss decreases to  0.8388320411859262\n",
      "The norm of grad vector is  0.519118026584702\n",
      "Loss decreases to  0.8385626196430828\n",
      "The norm of grad vector is  0.5188792280100692\n",
      "Loss decreases to  0.8382934459140566\n",
      "The norm of grad vector is  0.5186405448183563\n",
      "Loss decreases to  0.8380245197651708\n",
      "The norm of grad vector is  0.5184019769450914\n",
      "Loss decreases to  0.8377558409629821\n",
      "The norm of grad vector is  0.5181635243260527\n",
      "Loss decreases to  0.8374874092742775\n",
      "The norm of grad vector is  0.5179251868972614\n",
      "Loss decreases to  0.8372192244660761\n",
      "The norm of grad vector is  0.5176869645949792\n",
      "Loss decreases to  0.8369512863056263\n",
      "The norm of grad vector is  0.5174488573557018\n",
      "Loss decreases to  0.8366835945604089\n",
      "The norm of grad vector is  0.5172108651161547\n",
      "Loss decreases to  0.8364161489981311\n",
      "The norm of grad vector is  0.5169729878132883\n",
      "Loss decreases to  0.8361489493867325\n",
      "The norm of grad vector is  0.5167352253842736\n",
      "Loss decreases to  0.8358819954943798\n",
      "The norm of grad vector is  0.5164975777664983\n",
      "Loss decreases to  0.8356152870894686\n",
      "The norm of grad vector is  0.5162600448975608\n",
      "Loss decreases to  0.8353488239406234\n",
      "The norm of grad vector is  0.5160226267152684\n",
      "Loss decreases to  0.8350826058166947\n",
      "The norm of grad vector is  0.5157853231576299\n",
      "Loss decreases to  0.8348166324867612\n",
      "The norm of grad vector is  0.5155481341628558\n",
      "Loss decreases to  0.8345509037201283\n",
      "The norm of grad vector is  0.5153110596693508\n",
      "Loss decreases to  0.8342854192863278\n",
      "The norm of grad vector is  0.5150740996157104\n",
      "Loss decreases to  0.8340201789551179\n",
      "The norm of grad vector is  0.5148372539407192\n",
      "Loss decreases to  0.8337551824964817\n",
      "The norm of grad vector is  0.5146005225833452\n",
      "Loss decreases to  0.8334904296806277\n",
      "The norm of grad vector is  0.5143639054827374\n",
      "Loss decreases to  0.8332259202779901\n",
      "The norm of grad vector is  0.5141274025782206\n",
      "Loss decreases to  0.8329616540592266\n",
      "The norm of grad vector is  0.5138910138092949\n",
      "Loss decreases to  0.8326976307952196\n",
      "The norm of grad vector is  0.5136547391156288\n",
      "Loss decreases to  0.8324338502570751\n",
      "The norm of grad vector is  0.5134185784370588\n",
      "Loss decreases to  0.8321703122161223\n",
      "The norm of grad vector is  0.5131825317135846\n",
      "Loss decreases to  0.831907016443914\n",
      "The norm of grad vector is  0.5129465988853666\n",
      "Loss decreases to  0.8316439627122251\n",
      "The norm of grad vector is  0.5127107798927224\n",
      "Loss decreases to  0.8313811507930532\n",
      "The norm of grad vector is  0.5124750746761243\n",
      "Loss decreases to  0.8311185804586176\n",
      "The norm of grad vector is  0.5122394831761956\n",
      "Loss decreases to  0.83085625148136\n",
      "The norm of grad vector is  0.5120040053337092\n",
      "Loss decreases to  0.8305941636339423\n",
      "The norm of grad vector is  0.5117686410895832\n",
      "Loss decreases to  0.8303323166892481\n",
      "The norm of grad vector is  0.5115333903848786\n",
      "Loss decreases to  0.8300707104203817\n",
      "The norm of grad vector is  0.5112982531607974\n",
      "Loss decreases to  0.829809344600667\n",
      "The norm of grad vector is  0.511063229358679\n",
      "Loss decreases to  0.829548219003649\n",
      "The norm of grad vector is  0.5108283189199985\n",
      "Loss decreases to  0.8292873334030909\n",
      "The norm of grad vector is  0.5105935217863634\n",
      "Loss decreases to  0.8290266875729768\n",
      "The norm of grad vector is  0.5103588378995119\n",
      "Loss decreases to  0.8287662812875084\n",
      "The norm of grad vector is  0.5101242672013093\n",
      "Loss decreases to  0.8285061143211072\n",
      "The norm of grad vector is  0.509889809633748\n",
      "Loss decreases to  0.8282461864484124\n",
      "The norm of grad vector is  0.5096554651389426\n",
      "Loss decreases to  0.8279864974442819\n",
      "The norm of grad vector is  0.509421233659129\n",
      "Loss decreases to  0.8277270470837901\n",
      "The norm of grad vector is  0.5091871151366621\n",
      "Loss decreases to  0.8274678351422308\n",
      "The norm of grad vector is  0.508953109514014\n",
      "Loss decreases to  0.8272088613951127\n",
      "The norm of grad vector is  0.5087192167337715\n",
      "Loss decreases to  0.8269501256181634\n",
      "The norm of grad vector is  0.5084854367386333\n",
      "Loss decreases to  0.8266916275873257\n",
      "The norm of grad vector is  0.50825176947141\n",
      "Loss decreases to  0.8264333670787594\n",
      "The norm of grad vector is  0.5080182148750203\n",
      "Loss decreases to  0.8261753438688404\n",
      "The norm of grad vector is  0.5077847728924899\n",
      "Loss decreases to  0.8259175577341593\n",
      "The norm of grad vector is  0.5075514434669499\n",
      "Loss decreases to  0.8256600084515225\n",
      "The norm of grad vector is  0.507318226541634\n",
      "Loss decreases to  0.8254026957979522\n",
      "The norm of grad vector is  0.507085122059878\n",
      "Loss decreases to  0.8251456195506848\n",
      "The norm of grad vector is  0.5068521299651172\n",
      "Loss decreases to  0.8248887794871714\n",
      "The norm of grad vector is  0.5066192502008845\n",
      "Loss decreases to  0.8246321753850768\n",
      "The norm of grad vector is  0.5063864827108103\n",
      "Loss decreases to  0.8243758070222809\n",
      "The norm of grad vector is  0.5061538274386191\n",
      "Loss decreases to  0.824119674176876\n",
      "The norm of grad vector is  0.5059212843281284\n",
      "Loss decreases to  0.8238637766271694\n",
      "The norm of grad vector is  0.5056888533232479\n",
      "Loss decreases to  0.8236081141516799\n",
      "The norm of grad vector is  0.5054565343679769\n",
      "Loss decreases to  0.8233526865291402\n",
      "The norm of grad vector is  0.5052243274064043\n",
      "Loss decreases to  0.8230974935384953\n",
      "The norm of grad vector is  0.5049922323827051\n",
      "Loss decreases to  0.8228425349589031\n",
      "The norm of grad vector is  0.5047602492411416\n",
      "Loss decreases to  0.8225878105697322\n",
      "The norm of grad vector is  0.5045283779260584\n",
      "Loss decreases to  0.8223333201505644\n",
      "The norm of grad vector is  0.5042966183818859\n",
      "Loss decreases to  0.8220790634811924\n",
      "The norm of grad vector is  0.5040649705531343\n",
      "Loss decreases to  0.8218250403416208\n",
      "The norm of grad vector is  0.5038334343843953\n",
      "Loss decreases to  0.8215712505120645\n",
      "The norm of grad vector is  0.5036020098203394\n",
      "Loss decreases to  0.8213176937729495\n",
      "The norm of grad vector is  0.5033706968057156\n",
      "Loss decreases to  0.8210643699049123\n",
      "The norm of grad vector is  0.5031394952853496\n",
      "Loss decreases to  0.8208112786887996\n",
      "The norm of grad vector is  0.5029084052041423\n",
      "Loss decreases to  0.8205584199056686\n",
      "The norm of grad vector is  0.5026774265070698\n",
      "Loss decreases to  0.8203057933367853\n",
      "The norm of grad vector is  0.5024465591391815\n",
      "Loss decreases to  0.8200533987636265\n",
      "The norm of grad vector is  0.5022158030455991\n",
      "Loss decreases to  0.819801235967877\n",
      "The norm of grad vector is  0.5019851581715148\n",
      "Loss decreases to  0.8195493047314317\n",
      "The norm of grad vector is  0.5017546244621925\n",
      "Loss decreases to  0.8192976048363936\n",
      "The norm of grad vector is  0.5015242018629643\n",
      "Loss decreases to  0.8190461360650744\n",
      "The norm of grad vector is  0.5012938903192307\n",
      "Loss decreases to  0.8187948981999942\n",
      "The norm of grad vector is  0.5010636897764591\n",
      "Loss decreases to  0.8185438910238808\n",
      "The norm of grad vector is  0.500833600180184\n",
      "Loss decreases to  0.8182931143196698\n",
      "The norm of grad vector is  0.5006036214760039\n",
      "Loss decreases to  0.8180425678705059\n",
      "The norm of grad vector is  0.5003737536095835\n",
      "Loss decreases to  0.8177922514597387\n",
      "The norm of grad vector is  0.5001439965266493\n",
      "Loss decreases to  0.8175421648709266\n",
      "The norm of grad vector is  0.4999143501729916\n",
      "Loss decreases to  0.8172923078878345\n",
      "The norm of grad vector is  0.49968481449446206\n",
      "Loss decreases to  0.8170426802944335\n",
      "The norm of grad vector is  0.49945538943697365\n",
      "Loss decreases to  0.8167932818749014\n",
      "The norm of grad vector is  0.4992260749464988\n",
      "Loss decreases to  0.8165441124136223\n",
      "The norm of grad vector is  0.49899687096906997\n",
      "Loss decreases to  0.8162951716951862\n",
      "The norm of grad vector is  0.4987677774507783\n",
      "Loss decreases to  0.8160464595043877\n",
      "The norm of grad vector is  0.49853879433777176\n",
      "Loss decreases to  0.8157979756262291\n",
      "The norm of grad vector is  0.49830992157625653\n",
      "Loss decreases to  0.8155497198459157\n",
      "The norm of grad vector is  0.4980811591124946\n",
      "Loss decreases to  0.8153016919488597\n",
      "The norm of grad vector is  0.49785250689280397\n",
      "Loss decreases to  0.8150538917206764\n",
      "The norm of grad vector is  0.4976239648635569\n",
      "Loss decreases to  0.8148063189471868\n",
      "The norm of grad vector is  0.4973955329711812\n",
      "Loss decreases to  0.8145589734144163\n",
      "The norm of grad vector is  0.4971672111621566\n",
      "Loss decreases to  0.8143118549085928\n",
      "The norm of grad vector is  0.4969389993830173\n",
      "Loss decreases to  0.8140649632161502\n",
      "The norm of grad vector is  0.4967108975803489\n",
      "Loss decreases to  0.813818298123725\n",
      "The norm of grad vector is  0.49648290570078907\n",
      "Loss decreases to  0.8135718594181569\n",
      "The norm of grad vector is  0.49625502369102664\n",
      "Loss decreases to  0.8133256468864899\n",
      "The norm of grad vector is  0.49602725149780086\n",
      "Loss decreases to  0.8130796603159698\n",
      "The norm of grad vector is  0.4957995890679007\n",
      "Loss decreases to  0.8128338994940458\n",
      "The norm of grad vector is  0.49557203634816527\n",
      "Loss decreases to  0.8125883642083701\n",
      "The norm of grad vector is  0.4953445932854815\n",
      "Loss decreases to  0.8123430542467958\n",
      "The norm of grad vector is  0.49511725982678545\n",
      "Loss decreases to  0.8120979693973797\n",
      "The norm of grad vector is  0.49489003591905967\n",
      "Loss decreases to  0.81185310944838\n",
      "The norm of grad vector is  0.4946629215093359\n",
      "Loss decreases to  0.8116084741882559\n",
      "The norm of grad vector is  0.4944359165446907\n",
      "Loss decreases to  0.8113640634056695\n",
      "The norm of grad vector is  0.4942090209722481\n",
      "Loss decreases to  0.8111198768894825\n",
      "The norm of grad vector is  0.4939822347391773\n",
      "Loss decreases to  0.810875914428759\n",
      "The norm of grad vector is  0.4937555577926929\n",
      "Loss decreases to  0.8106321758127636\n",
      "The norm of grad vector is  0.4935289900800546\n",
      "Loss decreases to  0.8103886608309615\n",
      "The norm of grad vector is  0.4933025315485657\n",
      "Loss decreases to  0.8101453692730174\n",
      "The norm of grad vector is  0.49307618214557386\n",
      "Loss decreases to  0.8099023009287982\n",
      "The norm of grad vector is  0.4928499418184707\n",
      "Loss decreases to  0.8096594555883694\n",
      "The norm of grad vector is  0.4926238105146905\n",
      "Loss decreases to  0.809416833041996\n",
      "The norm of grad vector is  0.4923977881817096\n",
      "Loss decreases to  0.8091744330801436\n",
      "The norm of grad vector is  0.4921718747670476\n",
      "Loss decreases to  0.808932255493477\n",
      "The norm of grad vector is  0.4919460702182659\n",
      "Loss decreases to  0.8086903000728597\n",
      "The norm of grad vector is  0.4917203744829656\n",
      "Loss decreases to  0.8084485666093544\n",
      "The norm of grad vector is  0.49149478750879166\n",
      "Loss decreases to  0.8082070548942228\n",
      "The norm of grad vector is  0.49126930924342765\n",
      "Loss decreases to  0.8079657647189245\n",
      "The norm of grad vector is  0.49104393963459847\n",
      "Loss decreases to  0.8077246958751187\n",
      "The norm of grad vector is  0.4908186786300685\n",
      "Loss decreases to  0.8074838481546609\n",
      "The norm of grad vector is  0.4905935261776419\n",
      "Loss decreases to  0.8072432213496067\n",
      "The norm of grad vector is  0.4903684822251624\n",
      "Loss decreases to  0.8070028152522075\n",
      "The norm of grad vector is  0.4901435467205128\n",
      "Loss decreases to  0.8067626296549141\n",
      "The norm of grad vector is  0.4899187196116141\n",
      "Loss decreases to  0.8065226643503725\n",
      "The norm of grad vector is  0.48969400084642584\n",
      "Loss decreases to  0.8062829191314275\n",
      "The norm of grad vector is  0.489469390372946\n",
      "Loss decreases to  0.8060433937911204\n",
      "The norm of grad vector is  0.48924488813920936\n",
      "Loss decreases to  0.8058040881226889\n",
      "The norm of grad vector is  0.48902049409328907\n",
      "Loss decreases to  0.805565001919568\n",
      "The norm of grad vector is  0.48879620818329533\n",
      "Loss decreases to  0.8053261349753881\n",
      "The norm of grad vector is  0.48857203035737456\n",
      "Loss decreases to  0.8050874870839761\n",
      "The norm of grad vector is  0.4883479605637102\n",
      "Loss decreases to  0.8048490580393554\n",
      "The norm of grad vector is  0.4881239987505221\n",
      "Loss decreases to  0.8046108476357441\n",
      "The norm of grad vector is  0.487900144866066\n",
      "Loss decreases to  0.8043728556675567\n",
      "The norm of grad vector is  0.4876763988586333\n",
      "Loss decreases to  0.8041350819294029\n",
      "The norm of grad vector is  0.4874527606765517\n",
      "Loss decreases to  0.8038975262160872\n",
      "The norm of grad vector is  0.4872292302681829\n",
      "Loss decreases to  0.8036601883226089\n",
      "The norm of grad vector is  0.48700580758192447\n",
      "Loss decreases to  0.8034230680441629\n",
      "The norm of grad vector is  0.4867824925662095\n",
      "Loss decreases to  0.803186165176138\n",
      "The norm of grad vector is  0.4865592851695038\n",
      "Loss decreases to  0.8029494795141175\n",
      "The norm of grad vector is  0.4863361853403089\n",
      "Loss decreases to  0.8027130108538792\n",
      "The norm of grad vector is  0.4861131930271605\n",
      "Loss decreases to  0.8024767589913938\n",
      "The norm of grad vector is  0.4858903081786274\n",
      "Loss decreases to  0.8022407237228276\n",
      "The norm of grad vector is  0.48566753074331276\n",
      "Loss decreases to  0.8020049048445382\n",
      "The norm of grad vector is  0.48544486066985315\n",
      "Loss decreases to  0.8017693021530794\n",
      "The norm of grad vector is  0.48522229790691795\n",
      "Loss decreases to  0.8015339154451953\n",
      "The norm of grad vector is  0.4849998424032104\n",
      "Loss decreases to  0.8012987445178249\n",
      "The norm of grad vector is  0.48477749410746596\n",
      "Loss decreases to  0.8010637891680995\n",
      "The norm of grad vector is  0.484555252968453\n",
      "Loss decreases to  0.800829049193343\n",
      "The norm of grad vector is  0.4843331189349727\n",
      "Loss decreases to  0.8005945243910714\n",
      "The norm of grad vector is  0.48411109195585794\n",
      "Loss decreases to  0.800360214558994\n",
      "The norm of grad vector is  0.48388917197997455\n",
      "Loss decreases to  0.8001261194950106\n",
      "The norm of grad vector is  0.4836673589562194\n",
      "Loss decreases to  0.7998922389972142\n",
      "The norm of grad vector is  0.48344565283352253\n",
      "Loss decreases to  0.7996585728638889\n",
      "The norm of grad vector is  0.4832240535608439\n",
      "Loss decreases to  0.7994251208935101\n",
      "The norm of grad vector is  0.4830025610871765\n",
      "Loss decreases to  0.7991918828847451\n",
      "The norm of grad vector is  0.48278117536154347\n",
      "Loss decreases to  0.7989588586364517\n",
      "The norm of grad vector is  0.48255989633299995\n",
      "Loss decreases to  0.798726047947679\n",
      "The norm of grad vector is  0.48233872395063127\n",
      "Loss decreases to  0.7984934506176665\n",
      "The norm of grad vector is  0.4821176581635548\n",
      "Loss decreases to  0.7982610664458447\n",
      "The norm of grad vector is  0.481896698920917\n",
      "Loss decreases to  0.798028895231834\n",
      "The norm of grad vector is  0.48167584617189624\n",
      "Loss decreases to  0.7977969367754454\n",
      "The norm of grad vector is  0.4814550998657008\n",
      "Loss decreases to  0.7975651908766791\n",
      "The norm of grad vector is  0.4812344599515692\n",
      "Loss decreases to  0.7973336573357264\n",
      "The norm of grad vector is  0.48101392637877066\n",
      "Loss decreases to  0.7971023359529665\n",
      "The norm of grad vector is  0.4807934990966029\n",
      "Loss decreases to  0.7968712265289696\n",
      "The norm of grad vector is  0.48057317805439553\n",
      "Loss decreases to  0.7966403288644944\n",
      "The norm of grad vector is  0.4803529632015063\n",
      "Loss decreases to  0.7964096427604886\n",
      "The norm of grad vector is  0.48013285448732357\n",
      "Loss decreases to  0.7961791680180889\n",
      "The norm of grad vector is  0.4799128518612647\n",
      "Loss decreases to  0.7959489044386203\n",
      "The norm of grad vector is  0.47969295527277644\n",
      "Loss decreases to  0.7957188518235973\n",
      "The norm of grad vector is  0.4794731646713348\n",
      "Loss decreases to  0.7954890099747217\n",
      "The norm of grad vector is  0.47925348000644585\n",
      "Loss decreases to  0.7952593786938837\n",
      "The norm of grad vector is  0.47903390122764306\n",
      "Loss decreases to  0.7950299577831618\n",
      "The norm of grad vector is  0.4788144282844901\n",
      "Loss decreases to  0.7948007470448218\n",
      "The norm of grad vector is  0.47859506112657907\n",
      "Loss decreases to  0.7945717462813173\n",
      "The norm of grad vector is  0.4783757997035308\n",
      "Loss decreases to  0.7943429552952892\n",
      "The norm of grad vector is  0.47815664396499424\n",
      "Loss decreases to  0.7941143738895656\n",
      "The norm of grad vector is  0.4779375938606477\n",
      "Loss decreases to  0.7938860018671616\n",
      "The norm of grad vector is  0.4777186493401978\n",
      "Loss decreases to  0.7936578390312796\n",
      "The norm of grad vector is  0.47749981035337846\n",
      "Loss decreases to  0.7934298851853077\n",
      "The norm of grad vector is  0.47728107684995325\n",
      "Loss decreases to  0.7932021401328215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h = linear_regression_MultipleD()\n",
    "\n",
    "h.set_learning_rate(0.001)\n",
    "\n",
    "h.fit(x_train, y_train, iteration=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a loss value for each update being made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(h.loss_values) + 1), h.loss_values)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Iteration')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_values = np.dot(x_test, h.w[:-1]) + h.w[-1]\n",
    "\n",
    "plt.plot(predicted_values, y_test, 'ro', label='Data')\n",
    "plt.plot(np.sort(predicted_values), np.sort(predicted_values), linestyle='solid', label='Linear Regression')\n",
    "\n",
    "plt.xlabel('Predicted PTS')\n",
    "plt.ylabel('Actual PTS')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used RMSE (Root Mean Square Error) to measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9353636412340239\n"
     ]
    }
   ],
   "source": [
    "rmse = h.calculate_rmse(predicted_values, y_test)\n",
    "print(\"RMSE:\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
